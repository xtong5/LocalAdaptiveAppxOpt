\documentclass[review]{elsarticle}

\usepackage{amsmath,amssymb,amsthm,xspace,mathtools,hyperref,color}
\usepackage{mathrsfs,verbatim,xspace}


\input{FJHDef.tex}

\journal{Journal of Complexity}

%%%%%%%%%%%%%%%%%%%%%%%
%% Elsevier bibliography styles
%%%%%%%%%%%%%%%%%%%%%%%
%% To change the style, put a % in front of the second line of the current style and
%% remove the % from the second line of the style you would like to use.
%%%%%%%%%%%%%%%%%%%%%%%

%% Numbered
%\bibliographystyle{model1-num-names}

%% Numbered without titles
%\bibliographystyle{model1a-num-names}

%% Harvard
%\bibliographystyle{model2-names.bst}\biboptions{authoryear}

%% Vancouver numbered
%\usepackage{numcompress}\bibliographystyle{model3-num-names}

%% Vancouver name/year
\usepackage{numcompress}\bibliographystyle{model4-names}\biboptions{authoryear}

%% APA style
%\bibliographystyle{model5-names}\biboptions{authoryear}

%% AMA style
%\usepackage{numcompress}\bibliographystyle{model6-num-names}

%% `Elsevier LaTeX' style
%\bibliographystyle{elsarticle-num}
%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\abstol}{\varepsilon_{\textrm{a}}}
\newcommand{\datasites}{\{x_i\}_{i=0}^n}
\makeatletter
\newcommand{\vast}{\bBigg@{4}}
\newcommand{\Vast}{\bBigg@{6}}
\makeatother
%\newdefinition{algo}{Algorithm}
\theoremstyle{definition}
\newtheorem*{algoA}{Algorithm $A$}
\newcommand{\vastl}{\mathopen\vast}
\newcommand{\vastm}{\mathrel\vast}
\newcommand{\vastr}{\mathclose\vast}
\newcommand{\Vastl}{\mathopen\Vast}
\newcommand{\Vastm}{\mathrel\Vast}
\newcommand{\Vastr}{\mathclose\Vast}

\definecolor{orange}{rgb}{1.0,0.3,0.0}
\definecolor{violet}{rgb}{0.75,0,1}
\definecolor{darkgreen}{rgb}{0,0.6,0}
\newcommand{\frednote}[1]{  {\textcolor{red}  {\mbox{**Fred:} #1}}}
\newcommand{\yuhannote}[1]{  {\textcolor{darkgreen}  {\mbox{**Yuhan:} #1}}}
\newcommand{\xinnote}[1]{ {\textcolor{violet}  {\mbox{**Xin:} #1}}}
\newcommand{\scnote}[1]{ {\textcolor{orange}  {\mbox{**SC:} #1}}}

\newcommand{\Ixl}{I_{x,l}}
\newcommand{\Ixlx}{I_{x,\ell(x)}}
\newcommand{\Ixrlx}{I_{x,\rell(x)}}
\newcommand{\Ixhlx}{I_{x,\hell(x)}}
\newcommand{\hell}{\hat{\ell}}
\newcommand{\tell}{\tilde{\ell}}
\newcommand{\rell}{\mathring{\ell}}
\DeclareMathOperator{\lo}{lo}
\DeclareMathOperator{\ninit}{ninit}
\DeclareMathOperator{\errest}{errest}
\newtheorem{theorem}{Theorem}
\newtheorem{exmp}{Example}
\newtheorem{prop}[theorem]{Proposition}
\newcommand{\funappxg}{{\texttt{funappx\_g} \xspace}}
\newcommand{\funming}{{\texttt{funmin\_g} \xspace}}
\newcommand{\integralg}{{\texttt{integral\_g} \xspace}}
\newcommand{\cosappx}{\operatorname*{cosappx}}
\newcommand{\sinappx}{\operatorname*{sinappx}}
\begin{document}

\begin{frontmatter}

\title{Local Adpation for Approximation and Optimization of Univariate Functions}


%% Group authors per affiliation:
\author{Sou-Cheng Choi}
\author{Yuhan Ding}
\author{Fred J. Hickernell}
\author{Xin Tong}
\address{Department of Applied Mathematics, Illinois Institute of Technology, RE 208, 10 West 32$^{\text{nd}}$ Street, Chicago, Illinois, 60616, USA}

\begin{abstract}
Some ideas to get us going
\end{abstract}

\begin{keyword}
\sep \sep
\MSC[2010]  \sep
\end{keyword}

\end{frontmatter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction} \label{sec:intro}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The goal is to solve two problems by locally adaptive algorithms. For some suitable set, $\cc$, of continuous,
real-valued functions on the finite interval $[a,b]$, we  construct
algorithms $A:(\cc,(0,\infty)) \to L^{\infty}[a,b]$ and $M: (\cc,(0,\infty)) \to
\reals$ such that for any $f \in \cc$ and any error tolerance $\abstol > 0$,
\begin{gather}
\norm[\infty]{f - A(f,\abstol)} \le \abstol,  \tag{APP} \label{appxprob} \\
M(f,\abstol) - \min_{a \le x \le b} f(x)  \le \abstol. \tag{MIN} \label{optprob}
\end{gather}
The algorithms $A$ and $M$ depend only on function values. They choose the data sites in $[a,b]$ adaptively, depending on the function values already obtained at the data sites chosen previously.  The algorithms sample more densely where needed.  These
algorithms also automatically determine when to stop sampling the input function
and return the correct answers.

We demonstrate the following:
\begin{itemize}

\item The set $\cc$ is defined in Section \ref{sec:conedef} as a cone containing some functions whose second derivatives have finite sup-norms.  Drawing on Newton's divided diffrences in Section \ref{sec:ndd}, we construct a data-based upper bound on the sup-norms of these second derivatives and ultimately a data-based upper bound on the error of the linear spline in \eqref{appxerrbdb} towards the end of Section \ref{subsec:spline}.

\item Algorithms $A$ and $M$ are constructed in Sections \ref{subsec:appxalgo} and \ref{sec:minalgo}, which solve problems \eqref{appxprob} and \eqref{optprob}.  Guarantees of success are provided by Theorems  \ref{thm:algAworks} and \ref{thm:algMworks}.

\item Upper bounds on the computational costs of these algorithms  are derived in Sections \ref{subsec:appxcost} and \ref{subsec:optcost}.  The upper bounds in Theorems ??  are roughly proportional to $\Bignorm[1]{\sqrt{\abs{f''}}}/\sqrt{\abstol}$.

\item Lower bounds on the computational complexity of problems \eqref{appxprob} and \eqref{optprob} are derived in Sections \ref{subsec:appxcomp} and \ref{subsec:optcomp}.  The lower bounds in Theorems   are also proportional to $\Bignorm[1]{\sqrt{\abs{f''}}}/\sqrt{\abstol}$, which makes our algorithms essentially asymptotically optimal.

\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Cone, $\cc$} \label{sec:cone}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Linear splines form the basis for adaptive algorithms $A$ and $M$. The best linear spline error possible occurs for the Sobolev space of functions whose second derivatives have finite $L^{\infty}$-norm:
\[
\cw^{2,\infty}[a,b] : = \Bigl \{f \in C^1[a,b] : \norm{f''} : = \norm[{[a,b]}]{f''} : = \sup_{a \le x \le b} \abs{f''(x)} <  \infty \Bigr\}.
\]

To bound the error of the linear spline, our adaptive algorithms construct data-based upper bounds on $\norm[{[\alpha,
\beta]}]{f''}$ in terms of function values.  In \eqref{normbd} below, we derive an upper bound that depends only on values of $f$ sampled in the neighborhood of $[\alpha,
\beta]$. The set $\cc \subset \cw^{2,\infty}[a,b]$, for which this bound applies and for which our algorithms are guaranteed to succeed, consists of functions that are \emph{not too spiky}.  Our upper bound is homogeneous in the function, i.e., multiplying the function by $c$ multiplies the bound by $\abs{c}$.  Thus, $\cc$ is chosen to be a cone: $f \in \cc \implies cf \in \cc$ for all real
$c$. The precise definition of $\cc$ is given below in \eqref{conedef}. Cones of
functions are key to the theoretically justified adpative algorithms in
\cite{HicEtal14b}, \cite{Ton14a}, \cite{Din15a}, and \cite{HicRazYun15a}.

%--------------------------------------------------
\subsection{Newton's Divided Differences} \label{sec:ndd}
%--------------------------------------------------

Functions in $\cw^{2,\infty}[a,b]$ may have $f''(x)$ undefined at countably many $x \in [a,b]$.  Thus, we define $f''(x)$ as an interval-valued function:
\begin{gather*}
f''(x) := \Bigl[\liminf_{t \to x} f''(t), \ \limsup_{t \to x} f''(t) \Bigr], \\
 f''(]\alpha, \beta[) := \bigcup_ {x \in [\alpha, \beta]} f''(x), \qquad  ]\alpha, \beta[ \subset [a,b].
\end{gather*}
Furthermore, $\abs{f''(]\alpha, \beta[)}$ is the interval containing all values of $\abs{f''(x)}$ for $x \in ]\alpha, \beta[$.  We define a
measure of how small $\abs{f''(x)}$ can be for  $x \in ]\alpha, \beta[$ as follows:
\begin{equation} \label{minfppdef}
m(f,\alpha, \beta) = \inf \abs{f''(]\alpha, \beta[)}.
\end{equation}
It follows from this definition
that
\begin{equation} \label{mdec}
m(f,\alpha,\beta) \ge m(f,\gamma,\delta) \qquad \forall a \le \alpha \le \gamma \le \delta \le \beta \le b.
\end{equation}

We cannot determine $f''(x)$ for a specific $x$ based only on values of $f$. However,
we can use values of $f$ to provide an upper bound on $m(f,\alpha, \beta)$ via Newton divided differences.

Let $p$ denote the Lagrange quadratic interpolating polynomial at the nodes
$\{x_1, x_2, x_3\}$, which may be written as
\begin{equation*}
p(x) : = f[x_1] + f[x_1, x_2](x-x_1) + f[x_1, x_2, x_3](x-x_1)(x-x_2),
\end{equation*}
where the $f[\cdots]$ are the Newton divided differences; see for example~\cite{CheKin12a}. In particular,
\begin{gather}
\nonumber
f[x_1] = f(x_1), \qquad f[x_1, x_2] = \frac{f[x_2] - f[x_1]}{x_2-x_1},  \\
f[x_1, x_2,x_3] = \frac{f[x_2,x_3] - f[x_1,x_2]}{x_3-x_1}. \label{divdiff}
\end{gather}
For any $f \in
\cw^{2,\infty}[a,b]$, the function $f - p$ has at least three distinct zeros on
$[x_1, x_3]$, so $f' - p'$ has at least two distinct zeros on this interval,
i.e., there exist $\xi_\pm$ with $x_1 < \xi_- < x_2 < \xi_+ < x_3$ with
$f'(\xi_\pm) - p'(\xi_{\pm}) = 0$. If $f''$ is continuous, then we can conclude
that $ f[x_1, x_2, x_3]= p''(\zeta) =f''(\zeta) $ for some $\zeta \in ]x_1,
x_3[$. However, $\cw^{2,\infty}[a,b]$ contains functions without continuous
second derivatives. Fortunately, we can obtain a somewhat weaker---yet equally useful result---by  definition  \eqref{minfppdef}:
\begin{multline*}
\bigabs{f[x_1, x_2, x_3]}(\xi_+  - \xi_-) = \abs{p'(\xi_+) - p'(\xi_-)} =  \abs{f'(\xi_+) - f'(\xi_-)} \\
= \abs{\int_{\xi_-}^{\xi_+} f''(x) \, \dif x} \ge m(f,x_1, x_3) (\xi_+  - \xi_-).
\end{multline*}
So the the data-based second-order divided difference provides an upper
bound on how small $\abs{f''}$ can be in the interval $]x_1, x_3[$:
\begin{equation}
\bigabs{f[x_1, x_2, x_3]} \ge m(f,x_1, x_3).
\end{equation}

%--------------------------------------------------
\subsection{The Cone Definition}  \label{sec:conedef}
%--------------------------------------------------

Let $\fh$ be some positive number, and let $\fC : [0,b-a] \to [1,\infty)$ be any
non-decreasing function, which serves in our subsequent algorithms as inflation factors dependent on the grid size.
These parameters are used to define cone of functions for which our algorithms will
apply.  This cone only includes functions whose second derivatives do not change much in
magnitude over a short distance, namely,
\begin{multline} \label{conedef}
 \cc :=   \Bigl \{
 f  \in    \cw^{2,\infty}[a,b]:   \max\abs{f''(x)}  \le \tB(f,x,h_-,h_+)  \text{ for all } x \in [a,b],
\\ \text{with }  \max(h_{\pm}) \in ]0, \fh[  \Bigr \},
\end{multline}
where the pointwise upper bound, $\tB(f,x,h_-,h_+)$, is defined in terms of $x_{\pm} =x \pm h_{\pm}$ as follows:
\begin{multline*}
\tB(f,x,h_-,h_+)=\\
\begin{cases}
  \max\bigl(\fC(h_{-}) m(f,x_-,x),\fC(h_{+}) m(f,x,x_+)\bigr), & a \le x_- \le x_+ \le b,\\
\fC(h_{-}) m(f,x_-,x), & a \le x_- \le b <  x_+,\\
\fC(h_{+}) m(f,x,x_+), & x_- < a \le x_+ \le b.
\end{cases} %\quad
 %\Vastr
\end{multline*}
Either decreasing $\fh$ or increasing the function $\fC$ expands the cone to include more functions.  An example of $\fC$ is 
\begin{equation} \label{sampleC}
\fC(h) : = \frac{\fC(0) \fh}{\fh - h}, \quad h \in [0,\fh), \qquad \fC(0) > 1,\ \fh \le b - a.
\end{equation}

A function $f$ with $f''(\alpha) = f''(\beta) = \{0\} \ne f''((\alpha+\beta)/2)$ may
lie inside $\cc$ only if $\beta - \alpha > 2\fh$. Thus, $f''$ cannot have zeros too close to each other.  Except near the endpoints of
the interval, the definition of $\cc$ uses values of $f''$ on both sides of $x$
to bound $\max \abs{f''(x)}$. This allows $\cc$ to include functions with step
discontinuities in their second derivatives, provided that these discontinuities
do not occur too close to each other or too close to the ends of the interval.

We provide examples of functions lying outside $\cc$
and similar functions lying inside $\cc$. Consider these two functions defined
on $[-1,1]$ whose second derivatives oscillate wildly near $0$:
\begin{gather*}
f_1(x) = x^4 \sin(1/x), \\
 f_1''(x) = \begin{cases} (12x^2 - 1) \sin(1/x) -6 x \cos(1/x), & x \ne 0 \\
 [-1,1], & x = 0, \end{cases} \\
f_2(x) = 10  x^2 + f_1(x), \qquad f_2''(x) = 20+ f_1''(x).
\end{gather*}
These functions are plotted in Figure \ref{f1f2fig}. Because the $f''_1(x)$
takes on both signs for $x$ arbitrarily close to $0$ and on either side of $0$,
it follows that  $m(f_1,-h_-,0) = m(f_1,0,h_+) = 0$ for all $h_\pm \in [0,1]$.
However, $\max\abs{f''_1(0)} = 1$, so $f_1$ cannot lie inside
$\cc$ no matter how $\fh$ and $\fC$ are defined. On the other hand,
$m(f_2,\alpha, \beta) \ge 13.5$ for all $-1 \le \alpha < \beta \le 1$, and
$\max \abs{f''_2(x)} \le 27$ for all $x \in [-1,1]$, so $f_2 \in
\cc$ if $\fC(0) \ge 2$.

\begin{figure}[t]
\centering
\includegraphics[width=5.6cm]{f1f2plot.eps} \quad
\includegraphics[width=5.9cm]{f1closeplot.eps} \\
\includegraphics[width=5.6cm]{f1ppf2ppplot.eps} \quad
\includegraphics[width=5.9cm]{f2closeplot.eps}
\caption{The examples $f_1$ and $f_2$ and their second derivatives. Note that
$f_2''(x) = f_1''(x) + 20$.}
\label{f1f2fig}
\end{figure}

Consider the following function defined on $[-1,1]$, whose second derivative has jump discontinuities:
\begin{align*}
f_3(x) & = \begin{cases} \displaystyle
   \frac{1}{2\delta^2} \Bigl [4 \delta^2 + (x-c)^2 + (x-c-\delta)\abs{x-c-\delta}
\\ \qquad \qquad
    - (x-c+\delta)\abs{x-c+\delta} \Bigr ], & \abs{x-c} \le 2\delta,
\\ 0, & \text{otherwise},
\end{cases} \\
f''_3(x) & =
\begin{cases} \displaystyle
    \frac{1}{\delta^2} [1 + \sign(x-c-\delta) - \sign(x-c+\delta), & \abs{x-c} \le 2\delta,
\\ 0, & \text{otherwise}.
\end{cases}
\end{align*}
Here $c$ and $\delta$ are parameters satisfying $-1 \le c-2 \delta < c+ 2\delta \le 1$. This function and its second derivative
are shown in Figure \ref{f3fig} for $-c=\delta = 0.2$.  

If $\delta \ge 2 \fh$, then $f_3 \in \cc$ for
any choice of $\fC : [0,b-a] \to [1,\infty)$.  We see this by examining two cases.  For all $x \in [c - 2 \delta, c + 2 \delta]$ and all $h \in \, ]0,\fh[ \, \subseteq \, ]0,\delta/2[$, let $x_- = \max(a, x -h)$ and $x_+ = \min(x +h,b)$.  Then
\[
m(f_3,x_-,x) = m(f_3,x,x_+) = \sup_{-1 \le x \le 1} \abs{f_3''(x)}  = \delta^{-2}.
\]
For $x \in [-1,1] \setminus [c - 2 \delta , c + 2 \delta]$ we note that $f_3''(x) = 0$. Thus, the definition of the cone is satisfied.

However, if $\delta < 2 \fh$, then for $x = c-3\delta/2$ and $\delta/2 < h < \fh$, then regardless of how $\fc$ is defined,
\[
\fC(h)m(f_3,x - h,x)=\fC(h)m(f_3,x,x+h)=0 < \abs{f''_3(x)} = \delta^{-2},
\]
which violates the definition of $\cc$.  For $\delta < 2 \fh$ the function $f_3$ is too spiky to lie in the cone $\cc$.   This example illustrates how the choice of $\fh$ influences the width of
a spiky function that may or may not lie in $\cc$.

\begin{figure}[t]
\centering
\includegraphics[width=5.7cm]{f3plot.eps} \quad
\includegraphics[width=5.7cm]{f3ppplot.eps}
\caption{The example $f_3$ with $-c=\delta = 0.2$  and its piecwise constant second derivative.}
\label{f3fig}
\end{figure}

The above examples of functions outside $\cc$ have discontinuities in the second
derivative.  If a function has sufficient smoothness and the higher order derivatives are nicely behaved in a certain sense, then we can be sure that this function lies in $\cc$.   If  $f \in C^3[a,b]$ and for all $x \in [a,b]$, it happens that
\begin{equation} \label{conesuffconda}
\norm[{[\min(a,x-h),\max(x+h,b)]}]{f'''} \le \frac{\abs{f''(x)}}{h} \left( 1 - \frac{1}{\fC(h)} \right) \ \ \forall h \in [0,\fh],
\end{equation}
then one must have $f \in \cc$.  Using a Taylor expansion it follows that.
\begin{align*}
m(f,\min(a,x-h),\max(x+h,b)) & = \inf \abs{f''(]\min(a,x-h),\max(x+h,b)[)} \\
& \ge \abs{f''(x)}  - \norm[{[\min(a,x-h),\max(x+h,b)]}]{f'''}h \\
& \ge \abs{f''(x)}  - \abs{f''(x)}\left( 1 - \frac{1}{\fC(h)} \right) \\
& \ge \frac{\abs{f''(x)}}{\fC(h)} \qquad \forall x \in [a,b],\ h \in [0,\fh].
\end{align*}
Thus, $f \in \cc$ by the definition of the cone in \eqref{conedef}.

Sufficient condition \eqref{conesuffconda} fails if $f''(x) = 0$ for some $x$.  For $x \in ]a + \fh, b-\fh[$ one may replace \eqref{conesuffconda} by an alternative sufficient condition if  $f \in C^4[a,b]$:
\begin{multline} \label{conesuffcondb}
\max_{0 \le s(t-x) \le h }{\abs{f''''(t)}} \le \frac{2\abs{f''(x)}}{h^2} \left( 1 - \frac{1}{\fC(h)} \right) +  \frac{2\abs{f'''(x)}}{h}  \\ \forall  h \in [0,\fh], \ s = \sign(f''(x)f'''(x)).
\end{multline}
Note that here $s$ depends on $x$.  For a particular $x$, suppose that $s = +1$.  Then it follows by a Taylor expansion that
\begin{align*}
m(f,x,x+h) & = \inf \abs{f''(]x,x+h[)} \\
& \ge \inf_{x < t < x+h} \biggl\{ \abs{f''(x)} + \abs{f'''(x)}(t-x)  - \frac{\norm[{[x,x+h]}]{f''''}(t-x)^2}{2} \biggr\}\\
& \ge \inf_{x < t < x+h} \biggl\{ \abs{f''(x)} \biggl[ \frac{1}{\fC(h)} + \left(1 - \frac{(t-x)^2}{h^2}\right)\left(1 - \frac{1}{\fC(h)}\right) \biggr] \\
& \qquad \qquad + \abs{f'''(x)}(t-x)\left(1 -  \frac{t-x}{h}\right)  \biggr\}\\
& \ge  \frac{\abs{f''(x)}}{\fC(h)} \qquad \forall x \in [a,b],\ h \in [0,\fh].
\end{align*}
Thus, $f \in \cc$ by the definition of the cone in \eqref{conedef}.  A similar argument establishes the case $s = -1$.
%--------------------------------------------------
\subsection{The Linear Spline and Its Error} \label{subsec:spline}
%--------------------------------------------------

Define a \emph{partition} of an interval $[a, b]$, denoted $\datasites$, to be
an ordered sequence of points that includes the endpoints of the interval,
$a=:x_0 < x_1 < \cdots < x_{n-1} < x_{n}:=b$, where $x_{i+1}-x_{i-2} <\fh, \forall i=2,\ldots, n-1$.  The linear spline
approximation to a function $f$ based on the partition $\datasites$ is denoted,
$S(f,\datasites)$ and defined as
\begin{multline} \label{splinedef}
S(f,\datasites)(x) =  \frac{x-x_i}{x_{i-1} - x_i} f(x_{i-1}) + \frac{x-x_{i-1}}{x_{i} - x_{i-1}}f(x_i), \\ x_{i-1} \le x \le x_i, \ i=1, \ldots, n.
\end{multline}
The error of this approximation is bounded as follows \cite{??}
\begin{equation} \label{appxerrbda}
\norm{f - S(f,\datasites)} \le \max_{i=1, \ldots, n} \frac{(x_i - x_{i-1})^2\norm[{[x_{i-1},x_i]}]{f''}}{8}.
\end{equation}
To construct an adaptive algorithm, one requires an upper bound on
$\norm[{[x_{i-1},x_i]}]{f''}$ in terms of function values. This can be done for
$f \in \cc$ using the correspondence between a second order difference and the
second derivative in~\eqref{minfppdef}.

For all $ x \in [x_{i-1},x_i]$,  denote
\begin{align*}
&h_- = x - x_{i-3}, \qquad x_- = x_{i-3},  \qquad i=3,4,\ldots,n,\\
 &h_+ = x_{i+2} - x, \qquad x_+ =  x_{i+2}, \qquad i=1,2,\ldots,n-2.
\end{align*}
Let $\cp$ be a shorthand notation for the partition $\{x_i\}_{i=1}^\infty$, and define the following data based quantities.
\begin{align}\label{bpf}
B_{i+}(f,\cp)&:=\begin{cases}
    \fC(x_{i+2}-x_{i-1})\abs{f[x_i,x_{i+1},x_{i+2}]},  & i=1,\ldots,n-2,
\\ 0, & i=n-1,n.
\end{cases} \\
\label{bmf} 
 B_{i-}(f,\cp)&: =\begin{cases}
   0,  & i=1,2,
\\ \fC(x_{i}-x_{i-3})\abs{f[x_{i-3},x_{i-2},x_{i-1}])}, & i=3,\ldots,n,
\end{cases} \\
\nonumber
B_i(f,\cp) & : = \max\bigl(B_{i,\pm}(f,\cp) \bigr).
\end{align}
We now show that these give us a data-based upper bound on the norm of $f''$, namely,
\begin{equation}\label{normbd}
\norm[{[x_{i-1},x_i]}]{f''} \le B_i(f,\cp), \qquad i =1, \ldots, n.
\end{equation}

First, we consider the case of an interior interval, i.e., $i \in \{3, \ldots, n-2\}$. The definition of the cone implies that
\begin{equation*}
\abs{f''(x)} \le \max\bigl(\fC(h_-)m(f,x_{i-3},x),\fC(h_+)m(f,x,x_{i+2})\bigr)  \quad  \forall x \in [x_{i-1},x_i].
\end{equation*}
Applying the fact that $\fC$ is non-decreasing and property \eqref{mdec} yields
\begin{align*}
\nonumber
\MoveEqLeft{\norm[{[x_{i-1},x_i]}]{f''}}\\
\nonumber
 \le  & \sup_{x_{i-1} \le x \le x_i} \bigl[\max\bigl(\fC(x-x_{i-3})m(f,x_{i-3},x),\fC(x_{i+2}-x)m(f,x,x_{i+2})\bigr)\bigr]  \\
 %\nonumber & \qquad \qquad \times \max\bigl(m(f,x_{i-3},x),m(f,x,x_{i+2})\bigr] \\
\nonumber
 \le  &  \max\bigl(\fC(x_{i}-x_{i-3})m(f,x_{i-3},x_{i-1}),\fC(x_{i+2}-x_{i-1})m(f,x_i,x_{i+2})\bigr) \\
\nonumber  \le & \max\bigl(\fC(x_{i}-x_{i-3})\abs{f[x_{i-3},x_{i-2},x_{i-1}])},\bigr.
 \fC(x_{i+2}-x_{i-1})\abs{f[x_i,x_{i+1},x_{i+2}]}\bigr) \\
 =  & \max\bigl(B_{i,\pm}(f,\cp)\bigr) = B_i(f,\cp).
\end{align*}
A similar argument applied for the sub-intervals on the left and right borders of the interval. 
The bound in \eqref{normbd} combined with \eqref{appxerrbda} yield the
data-driven error bound for the linear spline:
\begin{equation} \label{appxerrbdb}
\norm{f - S(f,\datasites)} \le
\max_{i=1, \ldots, n} \frac{(x_i - x_{i-1})^2 B_{i}(f,\cp)}{8} .
\end{equation}
The goal is to increase the number of nodes in the partition as needed to make
this error bound smaller than the desired tolerance.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Function Approximation Algorithm, $A$}\label{sec:fappx}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Algorithm $A$} \label{subsec:appxalgo}
Instead of defining the cone in terms of $\fh$ directly, we choose an initial number of points
\begin{equation}
\label{nodefinition}
n_{\ninit} = \max\left\{\left\lceil n_{\text{hi}}
\left(\frac{n_{\lo}}{n_{\text{hi}}}\right)^{\frac{1}{1+b-a}}\right\rceil ,5\right\}.
\end{equation}
that lies somewhere between $n_{\lo}$ and $n_{\text{hi}}$, depending on the width $[a,b]$.  Then, we choose $\fh := (b-a)/n_{\ninit}$.  In this way, $n_{\ninit} + 1$ becomes the minimum number of sub-intervals used by the algorithm.  A higher $n_{\ninit}$ corresponds to a more robust algorithm.  By the arguments of the previous section, the following algorithm solves function approximation problem \eqref{appxprob}.

\begin{algoA} \label{AlgoA}
For some finite interval, $[a,b]$, some fixed integers $n_{\lo}, n_{\text{hi}}$ satisfying $5 \le n_{\lo} \le n_{\text{hi}}$, and some non-decreasing
$\fC:[0,b-a] \to [1, \infty)$, let $f:[a,b] \to \reals$ and $\abstol >0$ be
user inputs. Choose $\fh := (b-a)/n_{\ninit}$, where $n_{\ninit}$ is defined in \eqref{nodefinition}, and let  $n=n_{\ninit}+1$.
Define the partition, $\cp$ of  equally spaced points and the index set of intervals for which the error tolerance
is not yet satisified:
$$x_i=a+\frac{i}{n}(b-a), \ i=0,\ldots,n, \qquad
\mathcal{I} = \{1,2,\ldots,n-1,n\}.$$
Choose the inflation function $\fC$. Compute the second-order divided difference, $f[x_{i-1},
x_{i}, x_{i+1}]$, for $i = \{1,2,\ldots,n-1\}$. Then do the
following:
\begin{enumerate}[\bf Step 1.]%\hspace{8.5ex}
%\renewcommand{\labelenumi}{\textbf{Step \arabic{enumi}.}}
\item \label{stage1} Check for convergence.
Compute $B_{i,\pm}(f,\cp)$ as in \eqref{bpf} and \eqref{bmf}, where $i \in \mathcal{I}$.
Let
\[
\mathcal{I}_\pm = \left\{i \in \mathcal{I}: \frac{(x_i - x_{i-1})^2B_{i,\pm}(f,\cp)}{8}  > \abstol \right\}.
\]
Then update $\ci$ to be $\mathcal{I}_+ \cup \mathcal{I}_-$.  If $\mathcal{I} = \emptyset$, return the linear spline $A(f,\varepsilon) = S(f, \{x_i\}_{i=0}^n)$ and terminate the algorithm.
Otherwise, continue to the next step.
\item \label{stage2}
Denote $\widehat{\mathcal{I}}=\widehat{\mathcal{I}}_{+1} \cup \widehat{\mathcal{I}}_{+2} \cup \widehat{\mathcal{I}}_{-1} \cup \widehat{\mathcal{I}}_{-2},$ where
\begin{multline*}
\widehat{\mathcal{I}}_{\pm k} = \left\{i \in \{1,2,\ldots,n_0\}: \frac{(x_j - x_{j-1})^2B_{i,\pm}(f,\cp)}{8}  > \abstol,\right.\\
 \left.j=i\mp k, 0<j\le n_0, k=1,2. \right\},
\end{multline*}
\[\widetilde{\mathcal{I}}=\left\{i \in \mathcal{I} \cup \widehat{\mathcal{I}}: x_i - x_{i-1}=\max\limits_{j \in \mathcal{I} \cup \widehat{\mathcal{I}} } x_j-x_{j-1} \right\}.\]
Split in half those intervals $[x_{i-1},x_i]$ for which $i$ lies in $\widetilde{\mathcal{I}}$.
Update $n$, $\cp = \{x_i\}_{i=0}^n$, $\mathcal{I}$, and the $f[x_{i-1}, x_{i}, x_{i+1}]$ accordingly.  Return to Step~\ref{stage1}.
\end{enumerate}
\end{algoA}

\begin{theorem} \label{thm:algAworks}
Algorithm $A$ defined above solves problem \eqref{appxprob} for functions in the cone $\cc$ defined in \eqref{conedef}.
\end{theorem}

\subsection{The Computational Cost of $A$} \label{subsec:appxcost}

Next, we investigate the computational cost of our locally adaptive algorithm. Let $n_0= n_{\ninit} + 1$ denote the initial number of intervals, and
 \[
 h_0=\frac{b-a}{n_0} = \frac{b-a}{n_{\ninit}+1} = \frac{(b-a) \fh}{b - a + \fh}
 \]
be the initial width of the subintervals. To faciliate our derivation of the compuational cost of $A$, we introduce the new notation $\Ixl$, which is the unique half-open interval containing $x$ with
 the width $2^{-l}h_0$.
\[\Ixl :=\left[a+(j-1)2^{-l}h_0,a+j \ 2^{-l}h_0\right[, \quad j=\left\lceil\frac{(x-a)2^l}{h_0}\right\rceil, \ l \in \mathbb{N}_0, \ x \in [a,b[.\]
Let
$\ell(x)$ be defined such that
$I_{x,\ell(x)}$ is the final subinterval in algorithm $A$ that contains $x$ when the algorithm terminates.
Let $\bar{I}_{x,l}$ be a similar closed interval with generally five times the width:
\[\bar{I}_{x,l}=\left[a+\max(0,j-3)2^{-l}h_0, a+ \min(j+2,2^ln_0)2^{-l}h_0\right] \supset \Ixl,
\] with the same $j$ as above.  Let
\begin{equation}\label{eqn:defoflx}
L(x) = \min \left\{ l \in \{4, 5, \ldots\} :  \frac{1}{8} \fC\left(3\cdot2^{-l}h_0\right)(2^{-l}h_0)^2\norm[\bar{I}_{x,l}]{f''} \le \abstol \right\}.
\end{equation}
Note that $L(x)$ does depend on $f$, although this dependence is suppressed in the notation.

We now show that $\ell(x) \le L(x)$.  At each iteration of Algorithm $A$, $x$ lies in $\Ixl$ for some $l$, and that by the time algorithm $A$ terminates, all values of $l = 0, \ldots, \ell(x)$ are realized.  If $\ell(x) > L(x)$, then at some iteration $I_{x,L(x)}$ must be split in Step \ref{stage2} of $A$.  Let $\cp = \{x_i\}_{i=0}^n$ denote the partition of $[a,b]$ at this iteration, and then identify the closure of $I_{x,L(x)}$ as $[x_{i-1},x_i]$ for some $i$, where $x_i-x_{i-1}=2^{-L(x)}h_0$.
Referring to Step \ref{stage2} of $A$, it is necessary that $i \in \mathcal{I} \cap \widetilde{\mathcal{I}}$ or  $i \in \widehat{\mathcal{I}}\cap \widetilde{\mathcal{I}}$ for $I_{x,L(x)}$ to be split.  We show that neither of these conditions can hold, thus contradicting the assumption that $\ell(x) > L(x)$.

Let us investigate these two conditions separately.
\begin{enumerate}
  \item $i \in \mathcal{I} \cap \widetilde{\mathcal{I}}$.  If $i \in \mathcal{I}_+ \cap \widetilde{\mathcal{I}}$, then
  \begin{equation}
  \label{conditionone} x_{i}-x_{i-1} \ge x_{j}-x_{j-1}, \qquad   j=i+1,i+2, \qquad [x_i,x_{i+2}] \subseteq \bar{I}_{x,L(x)}.
  \end{equation}
  This implies that
  \begin{align*}
  \abstol & <  \frac{B_{i,+}(f,\cp)}{8}(x_i-x_{i-1})^2 \\
  & \le \frac{\fC(x_{i+2}-x_{i-1})f[x_{i},x_{i+1},x_{i+2}]}{8}(x_{i}-x_{i-1})^2 \qquad \text{by } \eqref{bpf} \\
  %\text{since } x_{i}-x_{i-1} \ge x_{j}-x_{j-1}, j=i+1,i+2 \ \ \
  &\le  \frac 1 8 \fC\left(3\cdot2^{-L(x)}h_0\right) (2^{-L(x)}h_0)^2\norm[{[x_{i},x_{i+2}]}]{f''} \qquad \text{by } \eqref{conditionone} \\
     & \le   \frac 1 8 \fC\left(3\cdot2^{-L(x)}h_0\right) (2^{-L(x)}h_0)^2  \norm[\bar{I}_{x,L(x)}]{f''}  \qquad \text{by } \eqref{conditionone} \\
     & \le    \abstol \qquad \text{by } \eqref{eqn:defoflx} .
  \end{align*}
A similar argument yields $\abstol < \abstol$ for $i \in \mathcal{I}_- \cap \widetilde{\mathcal{I}}$ as well.  Thus $i \in \mathcal{I} \cap \widetilde{\mathcal{I}}$ is impossible.
  \item $i \in \widehat{\mathcal{I}}\cap \widetilde{\mathcal{I}}$\\
  There are four different cases in this situation. We first consider $i \in \widehat{\mathcal{I}}_{+1} \cap \widetilde{\mathcal{I}}$.
  Thus
  \begin{equation} \label{conditiontwo}
  x_{i}-x_{i-1} \ge x_{j}-x_{j-1},\qquad j=i-1,i+1, \qquad [x_{i-1},x_{i+1}] \subseteq \bar{I}_{x,l}.
  \end{equation}
  Then
  \begin{align*}
  \abstol & < \frac{B_{i-1,+}(f,\cp)}{8}(x_{i-1}-x_{i-2})^2 \\
& \le  \frac{\fC(x_{i+1}-x_{i-2})f[x_{i-1},x_{i},x_{i+1}]}{8}(x_{i-1}-x_{i-2})^2 \qquad  \text{by } \eqref{bpf}   \\
 & \le  \frac 1 8 \fC\left(3\cdot2^{-L(x)}h_0\right) (2^{-L(x)}h_0)^2 \norm[{[x_{i-1},x_{i+1}]}]{f''}  \text{by } \eqref{conditiontwo}  \\ 
     & \le   \frac 1 8 \fC\left(3\cdot2^{-L(x)}h_0\right) (2^{-L(x)}h_0)^2 \norm[\bar{I}_{x,L(x)}]{f''} \qquad \text{by } \eqref{conditiontwo}  \\
     & \le  \abstol \qquad \text{by } \eqref{eqn:defoflx}.
  \end{align*}
  We also get contradiction for other three cases. So we cannot have $i \in \widehat{\mathcal{I}}\cap \widetilde{\mathcal{I}}$.
\end{enumerate}
Hence, we prove $\ell(x) \le L(x)$. This fact is used to prove an upper bound on the computational cost of Algorithm $A$.

\begin{theorem}\label{thm:cost}
Let $\cost(A,f,\abstol)$ denote the number of functional evaluations required by Algorithm $A$ for the input function $f$ and the absolute error tolerance $\abstol$.  This algorithm is known to successfully solve problem \eqref{appxprob} for functions in $\cc$.  The computational cost of doing so has the following upper bound:
\begin{equation*}
\cost(A,f,\abstol) \le \frac{1}{h_0}\int_a^b 2^{L(x)} \, \dif x +1 \\
\end{equation*}
where $L(x)$ is defined in \eqref{eqn:defoflx} and 
\begin{equation*}
2^{L(x)} = \min\left\{2^l:  2^l \ge \sqrt{\frac{\fC\left(3\cdot 2^{-l} h_0\right) h_0^2 \norm[\bar{I}_{x,l}]{f''} }{8 \abstol}}, \  l \in  \natzero\right\}.
\end{equation*}
\end{theorem}

\begin{proof}
Let $\cp=\{x_i\}_{i=0}^n$ be the final partition when $A(f,\abstol)$ successfully terminates. Note that $2^{\ell(x)}$ is constant on $I_{x_i,\ell(x_i)} = [x_{i},x_{i+1}[$ for $i=0, \ldots, n-1$.  Furthermore  $\int_{x_i}^{x_{i+1}} 2^{\ell(x)} \, \dif  x =  h_0$.  Then the number of function values requires is 
\begin{equation*}
n+1 = 1 + \sum_{i=0}^{n-1} 1 = 1 + \sum_{i=0}^{n-1} \frac{1}{h_0} \int_{x_i}^{x_{i+1}} 2^{\ell(x)} \, \dif  x = 1 + \frac{1}{h_0}\int_a^b 2^{\ell(x)} \, \dif x.
\end{equation*}
Noting that $\ell(x) \le L(x)$ establishes the formula for $\cost(A,f,\abstol)$.  The formula for $2^{L(x)}$ follows from the definition of $L(x)$ in \eqref{eqn:defoflx}.
\end{proof}

From Theorem~\ref{thm:cost}, we know for very small $\varepsilon$,
$L(x)$ tends to $\infty$, $ \norm[\bar{I}_{x,L(x)}]{f''} $ tends to $|f''(x)|$ and $3\cdot 2^{-L(x)}h_0$ tends to 0.
Thus we can have
$$ 2^{L(x)} \approx \sqrt{\frac{\fC\left(0\right)  h_0^2 |f''(x)|}{8\abstol}}.$$
The upper bound on computational cost tends to
\begin{align*}
\cost(S,f,\varepsilon)  & \lesssim \frac{n_0}{(b-a)}\int_a^b \sqrt{\frac{\fC\left(0\right)  h_0^2 |f''(x)|}{8\abstol}} \, \dif x +1 \\
& =\sqrt{\frac{\fC\left(0\right)}{8\abstol}}\int_a^b \sqrt{|f''(x)|} \, \dif x  +1\\
& =\sqrt{\frac{\fC\left(0\right)}{8\abstol}}\norm[1]{\sqrt{|f''|}} \, \dif x +1,
\end{align*}
where $\norm[1]{g}=\int_a^b |g(x)| \, \dif x$.
However, when $\abstol$ is not that small, the computational cost does not only depend on $\norm[1]{\sqrt{|f''|}}$ but also depends on where and how the peaky parts are located.\\

For functions in the cone $\cc$, the quantities $\norm{f''}$ and $\Bignorm[1]{\sqrt{\abs{f''}}}^2$ bound each other as is claimed in the following proposition.

\begin{prop} For all $f \in \cc$ it follows that
\begin{equation}
\max_{0 \le h \le \fh} \frac{h^2}{\fC(h)} \norm{f''} \le \Bignorm[1]{\sqrt{\abs{f''}}}^2 \le (b-a)^2 \norm{f''}.
\end{equation}
\end{prop}
\begin{proof}
The right hand side inequality is true for all $f$, not only those in $\cc$, and follows directly from the definitions of the norms.  Note that
\begin{equation} \label{onebdm}
\Bignorm[1]{\sqrt{\abs{f''}}}^2 \ge \biggl\{ \int_{\alpha}^{\beta} \sqrt{\abs{f''(x)}} \, \dif x \biggr\}^2 \ge  (\beta - \alpha)^2 m(f,\alpha,\beta)
\end{equation}
The left hand side inequality comes from the definition of the cone:
\begin{align*}
\norm{f''} & = \sup_{a \le x \le b} \max \abs{f''(x)} \\
& \le \sup_{a \le x \le b} \min\bigl\{\tB(f,x,h,h) : h \in [0,\fh], \ a \le x - h < x+h \le b \bigr \} \quad \text{by } \eqref{conedef} \\
& \le \min_{0 \le h \le \fh} \frac{\fC(h)}{h^2} \Bignorm[1]{\sqrt{\abs{f''}}}^2 \qquad \text{by } \eqref{conedef} \text{ and } \eqref{onebdm}.
\end{align*}

If $\fC$ is defined as in \eqref{sampleC}, then
\[
\max_{0 \le h \le \fh} \frac{h^2}{\fC(h)} = \max_{0 \le h \le \fh} \frac{h^2(\fh - h)}{\fC(0)\fh} = \frac{4\fh^2}{29\fC(0)}.
\]
When $\fh$ is small, it is possible for $\Bignorm[1]{\sqrt{\abs{f''}}}^2$ to be quite small in comparison to $\norm{f''}$.  This occurs when $f''$ is spiky. 
\end{proof}

\subsection{Lower Complexity Bound} \label{subsec:appxcomp}



\section{The Minimization Algorithm, $M$} \label{sec:funmin}

\subsection{Algorithm $M$}  \label{sec:minalgo}

\begin{theorem} \label{thm:algMworks}
Algorithm $M$ defined above solves problem \eqref{optprob} for functions in the cone $\cc$ defined in \eqref{conedef}.
\end{theorem}
\xinnote{
we need a theorem that works.\\
Numerical examples.\\
We need computational cost\\
complexity bound
}

\subsection{Computational Cost} \label{subsec:optcost}

\subsection{Lower Complexity Bound} \label{subsec:optcomp}



\section{Numerical Examples}

\input{examples}

\section{Discussion}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{References}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliography{FJH23,FJHOwn23}

\end{document}








