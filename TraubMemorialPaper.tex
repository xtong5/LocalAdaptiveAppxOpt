\documentclass[review]{elsarticle}

\usepackage{amsmath,amssymb,amsthm,xspace,mathtools,hyperref,color}
\usepackage{mathrsfs,verbatim,xspace}


\input{FJHDef.tex}

\journal{Journal of Complexity}

%%%%%%%%%%%%%%%%%%%%%%%
%% Elsevier bibliography styles
%%%%%%%%%%%%%%%%%%%%%%%
%% To change the style, put a % in front of the second line of the current style and
%% remove the % from the second line of the style you would like to use.
%%%%%%%%%%%%%%%%%%%%%%%

%% Numbered
%\bibliographystyle{model1-num-names}

%% Numbered without titles
%\bibliographystyle{model1a-num-names}

%% Harvard
%\bibliographystyle{model2-names.bst}\biboptions{authoryear}

%% Vancouver numbered
%\usepackage{numcompress}\bibliographystyle{model3-num-names}

%% Vancouver name/year
\usepackage{numcompress}\bibliographystyle{model4-names}\biboptions{authoryear}

%% APA style
%\bibliographystyle{model5-names}\biboptions{authoryear}

%% AMA style
%\usepackage{numcompress}\bibliographystyle{model6-num-names}

%% `Elsevier LaTeX' style
%\bibliographystyle{elsarticle-num}
%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\abstol}{\varepsilon_{\textrm{a}}}
\newcommand{\datasites}{\{x_i\}_{i=0}^n}
\makeatletter
\newcommand{\vast}{\bBigg@{4}}
\newcommand{\Vast}{\bBigg@{6}}
\makeatother
%\newdefinition{algo}{Algorithm}
\theoremstyle{definition}
\newtheorem*{algoA}{Algorithm $A$}
\newcommand{\vastl}{\mathopen\vast}
\newcommand{\vastm}{\mathrel\vast}
\newcommand{\vastr}{\mathclose\vast}
\newcommand{\Vastl}{\mathopen\Vast}
\newcommand{\Vastm}{\mathrel\Vast}
\newcommand{\Vastr}{\mathclose\Vast}

\definecolor{orange}{rgb}{1.0,0.3,0.0}
\definecolor{violet}{rgb}{0.75,0,1}
\definecolor{darkgreen}{rgb}{0,0.6,0}
\newcommand{\frednote}[1]{  {\textcolor{red}  {\mbox{**Fred:} #1}}}
\newcommand{\yuhannote}[1]{  {\textcolor{darkgreen}  {\mbox{**Yuhan:} #1}}}
\newcommand{\xinnote}[1]{ {\textcolor{violet}  {\mbox{**Xin:} #1}}}
\newcommand{\scnote}[1]{ {\textcolor{orange}  {\mbox{**SC:} #1}}}

\newcommand{\Ixl}{I_{x,l}}
\newcommand{\Ixlx}{I_{x,\ell(x)}}
\newcommand{\Ixrlx}{I_{x,\rell(x)}}
\newcommand{\Ixhlx}{I_{x,\hell(x)}}
\newcommand{\hell}{\hat{\ell}}
\newcommand{\tell}{\tilde{\ell}}
\newcommand{\rell}{\mathring{\ell}}
\DeclareMathOperator{\lo}{lo}
\DeclareMathOperator{\ninit}{ninit}
\DeclareMathOperator{\errest}{errest}
\newtheorem{theorem}{Theorem}
\newtheorem{exmp}{Example}
\newcommand{\funappxg}{{\texttt{funappx\_g} \xspace}}


\begin{document}

\begin{frontmatter}

\title{Local Adpation for Approximation and Optimization of Univariate Functions}


%% Group authors per affiliation:
\author{Sou-Cheng Choi}
\author{Yuhan Ding}
\author{Fred J. Hickernell}
\author{Xin Tong}
\address{Department of Applied Mathematics, Illinois Institute of Technology, RE 208, 10 West 32$^{\text{nd}}$ Street, Chicago, Illinois, 60616, USA}

\begin{abstract}
Some ideas to get us going
\end{abstract}

\begin{keyword}
\sep \sep
\MSC[2010]  \sep
\end{keyword}

\end{frontmatter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction} \label{sec:intro}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The goal is to solve two problems by locally adaptive algorithms. For some suitable set, $\cc$, of continuous,
real-valued functions on the finite interval $[a,b]$, we  construct
algorithms $A:(\cc,(0,\infty)) \to L^{\infty}[a,b]$ and $M: (\cc,(0,\infty)) \to
\reals$ such that for any $f \in \cc$ and any error tolerance $\abstol > 0$,
\begin{gather}
\norm[\infty]{f - A(f,\abstol)} \le \abstol,  \tag{APP} \label{appxprob} \\
M(f,\abstol) - \min_{a \le x \le b} f(x)  \le \abstol. \tag{MIN} \label{optprob}
\end{gather}
Here $A$ and $M$ depend only on function values. They choose the data sites in $[a,b]$ adaptively, depending on the function values already obtained at the data sites chosen previously.  The algorithms sample more densely in places where it is required.  These
algorithms also automatically determine when to stop sampling the input function
and return their answers.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Cone, $\cc$} \label{sec:cone}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Linear splines will form the basis for our adaptive algorithms $A$ and $M$. The best linear spline error possible occurs for the Sobolev space of functions whose second derivatives have finite $L^{\infty}$-norm:
\[
\cw^{2,\infty}[a,b] : = \Bigl \{f \in C^1[a,b] : \norm{f''} : = \norm[{[a,b]}]{f''} : = \sup_{a \le x \le b} \abs{f''(x)} <  \infty \Bigr\}.
\]

To bound the error of the linear spline, our adaptive algorithms construct data-based upper bounds on $\norm[{[\alpha,
\beta]}]{f''}$ in terms of function values.  In \eqref{normbd} below, we derive an upper bound $B$, such that
\[
\norm[{[\alpha, \beta]}]{f''} \le B(f,[\alpha,\beta]) \quad \forall f \in \cc, \ a \le \alpha < \beta \le b,
\]
where $B$ depends only on values of $f$ sampled in the neighborhood of $[\alpha,
\beta]$. The set $\cc \subset \cw^{2,\infty}[a,b]$, for which this bound applies and for which our algorithms are guaranteed to succeed, consists of functions that are \emph{not too spiky}.  Our upper bound is homogeneous in the function, i.e.,
$B(cf,[\alpha,\beta]) = \abs{c} B(f,[\alpha,\beta])$ for all real $c$. Thus, $\cc$ is chosen to be a cone: $f \in \cc \implies cf \in \cc$ for all real
$c$. The precise definition of $\cc$ is given below in \eqref{conedef}. Cones of
functions are key to the theoretically justified adpative algorithms in
\cite{HicEtal14b}, \cite{Ton14a}, \cite{Din15a}, and \cite{HicRazYun15a}.

%--------------------------------------------------
\subsection{Newton Divided Differences} \label{sec:ndd}
%--------------------------------------------------

Functions in $\cw^{2,\infty}[a,b]$ may have $f''(x)$ undefined at countably many $x \in [a,b]$.  Thus, we define $f''(x)$ as an interval-valued function:
\begin{gather*}
f''(x) := \Bigl[\liminf_{t \to x} f''(t), \limsup_{t \to x} f''(t) \Bigr], \\
 f''(]\alpha, \beta[) := \bigcup_ {x \in ]\alpha,\beta[} f''(x), \qquad  ]\alpha,\beta[ \subset [a,b].
\end{gather*}
Furthermore, $\abs{f''(]\alpha, \beta[)}$ is the interval containing all values of $\abs{f''(x)}$ for $x \in ]\alpha,\beta[$.  We define a
measure of how small $\abs{f''(x)}$ can be for  $x \in [\alpha, \beta]$ as follows:
\begin{equation} \label{minfppdef}
m(f,\alpha, \beta) = \inf \abs{f''(]\alpha, \beta[)}.
\end{equation}
For any open intervals $I , J \subseteq [a,b]$, it follows from the definition~\eqref{minfppdef}
that
\begin{equation} \label{mdec}
m(f,I) \ge m(f,J) \qquad \forall I \subseteq J.
\end{equation}

We cannot determine $f''(x)$ for a specific $x$ based only on values of $f$. However,
we can use values of $f$ to provide an upper bound on $m(f,\alpha, \beta)$ via the Newton divided differences.

Let $p$ denote the Lagrange quadratic interpolating polynomial at the nodes
$\{x_1, x_2, x_3\}$, which may be written as
\begin{equation*}
p(x) : = f[x_1] + f[x_1, x_2](x-x_1) + f[x_1, x_2, x_3](x-x_1)(x-x_2),
\end{equation*}
where the $f[\cdots]$ are the Newton divided differences; see for example~\cite{CheKin12a}. In particular,
\begin{gather}
\nonumber
f[x_1] = f(x_1), \qquad f[x_1, x_2] = \frac{f[x_2] - f[x_1]}{x_2-x_1},  \\
f[x_1, x_2,x_3] = \frac{f[x_2,x_3] - f[x_1,x_2]}{x_3-x_1}. \label{divdiff}
\end{gather}
For any $f \in
\cw^{2,\infty}[a,b]$, the function $f - p$ has at least three distinct zeros on
$[x_1, x_3]$, so $f' - p'$ has at least two distinct zeros on this interval,
i.e., there exist $\xi_\pm$ with $x_1 < \xi_- < x_2 < \xi_+ < x_3$ with
$f'(\xi_\pm) - p'(\xi_{\pm}) = 0$. If $f''$ is continuous, then we can conclude
that $ f[x_1, x_2, x_3]= p''(\zeta) =f''(\zeta) $ for some $\zeta \in [x_1,
x_3]$. However, $\cw^{2,\infty}[a,b]$ contains functions without continuous
second derivatives. Fortunately, we can obtain a somewhat weaker---yet equally useful result---by the definition of \eqref{minfppdef},
\begin{multline*}
\bigabs{f[x_1, x_2, x_3]}(\xi_+  - \xi_-) = \abs{p'(\xi_+) - p'(\xi_-)} =  \abs{f'(\xi_+) - f'(\xi_-)} \\
= \abs{\int_{\xi_-}^{\xi_+} f''(x) \, \dif x} \ge m(f,x_1, x_3) (\xi_+  - \xi_-).
\end{multline*}
So the the data-based second-order divided difference provides an upper
bound on how small $\abs{f''}$ can be in the interval $[x_1, x_3]$:
\begin{equation}
\bigabs{f[x_1, x_2, x_3]} \ge m(f,x_1, x_3).
\end{equation}

%--------------------------------------------------
\subsection{The Cone Definition}  \label{sec:conedef}
%--------------------------------------------------

Let $\fh$ be some positive number, and let $\fC : [0,b-a] \to [1,\infty)$ be any
non-decreasing function, which serves in our subsequent algorithms as inflation factors dependent on the grid size~$h$. 
The parameters are used to define cone of functions for which our algorithms will
apply.  This cone only includes functions whose second derivatives do not change much in
magnitude over a short distance, namely,
\begin{multline} \label{conedef}
 \cc :=   \Bigl \{
 f  \in    \cw^{2,\infty}[a,b]:   \max\abs{f''(x)}  \le \tB(f,x,h_-,h_+)  \text{ for all } x \in [a,b],
\\ \text{with } x_{\pm} =x \pm h_{\pm}, \  \max(h_{\pm}) \in [0, \fh]  \Bigr \},
\end{multline}
where 
\begin{multline*}
\tB(f,x,h_-,h_+)=\\
\begin{cases}
  \max\bigl(\fC(h_{-}) m(f,x_-,x),\fC(h_{+}) m(f,x,x_+)\bigr), & a \le x_- \le x_+ \le b,\\
\fC(h_{-}) m(f,x_-,x), & a \le x_- \le b <  x_+,\\
\fC(h_{+}) m(f,x,x_+), & x_- < a \le x_+ \le b.
\end{cases} %\quad
 %\Vastr
\end{multline*}
Increasing either $\fh$ or the function $\fC$ expands the cone to include more functions.

A function $f$ with $f''(\alpha) = f''(\beta) = \{0\} \ne f''((\alpha+\beta)/2)$ may
lie inside $\cc$ only if $\beta - \alpha > 2\fh$. Thus, $f''$ cannot have zeros two close to each other.  Except near the endpoints of
the interval, the definition of $\cc$ uses values of $f''$ on both sides of $x$
to bound $\max \abs{f''(x)}$. This allows $\cc$ to include functions with step
discontinuities in their second derivatives, provided that these discontinuities
do not occur too close to each other or too close to the ends of the interval.

We provide examples of functions lying outside $\cc$
and similar functions lying inside $\cc$. Consider these two functions defined
on $[-1,1]$ whose second derivatives oscillate wildly near $0$:
\begin{gather*}
f_1(x) = x^4 \sin(1/x), \\
 f_1''(x) = \begin{cases} (12x^2 - 1) \sin(1/x) -6 x \cos(1/x), & x \ne 0 \\
 [-1,1], & x = 0, \end{cases} \\
f_2(x) = 10  x^2 + f_1(x), \qquad f_2''(x) = 20+ f_1''(x).
\end{gather*}
These functions are plotted in Figure \ref{f1f2fig}. Because the $f''_1(x)$
takes on both signs for $x$ arbitrarily close to $0$ and on either side of $0$,
it follows that  $m(f_1,-h_-,0) = m(f_1,0,h_+) = 0$ for all $h_\pm \in [0,1]$.
However, $\max\abs{f''_1(0)} = 1$, so $f_1$ cannot lie inside
$\cc$ no matter how $\fh$ and $\fC$ are defined. On the other hand,
$m(f_2,\alpha, \beta) \ge 13.5$ for all $-1 \le \alpha < \beta \le 1$, and
$\max \abs{f''_2(x)} \le 27$ for all $x \in [-1,1]$, so $f_2 \in
\cc$ if $\fC(0) \ge 2$.

\begin{figure}[t]
\centering
\includegraphics[width=5.6cm]{f1f2plot.eps} \quad
\includegraphics[width=5.9cm]{f1closeplot.eps} \\
\includegraphics[width=5.6cm]{f1ppf2ppplot.eps} \quad
\includegraphics[width=5.9cm]{f2closeplot.eps}
\caption{The examples $f_1$ and $f_2$ and their second derivatives. Note that
$f_2''(x) = f_1''(x) + 20$.}
\label{f1f2fig}
\end{figure}

Consider the following function defined on $[-1,1]$, whose second derivative has jump discontinuities:
\begin{align*}
f_3(x) & = \begin{cases} \displaystyle
   \frac{1}{2\delta^2} \Bigl [4 \delta^2 + (x-c)^2 + (x-c-\delta)\abs{x-c-\delta}
\\ \qquad \qquad
    - (x-c+\delta)\abs{x-c+\delta} \Bigr ], & \abs{x-c} \le 2\delta,
\\ 0, & \text{otherwise},
\end{cases} \\
f''_3(x) & =
\begin{cases} \displaystyle
    \frac{1}{\delta^2} [1 + \sign(x-c-\delta) - \sign(x-c+\delta), & \abs{x-c} \le 2\delta,
\\ 0, & \text{otherwise}.
\end{cases}
\end{align*}
Here $c$ and $\delta$ are parameters. This function and its second derivative
are shown in Figure \ref{f3fig} for $-c=\delta = 0.2$. For this choice of
parameters, if $\fh < 0.1$, then for
any choice of $\fC : [0,b-a] \to [1,\infty)$ it follows that $f_3 \in \cc$.  We see this by examining two cases.  If $x \in [-0.6, 0.2]$, then for $h_\pm \in [0,\fh]$, and $x_\pm = x\pm h_\pm$, it follows that $-1 \le x_- \le x_+ \le 1$, and
\[
\max\bigl(m(f_3,x_-,x),m(f_3,x,x_+)\bigr) = \sup_{-1 \le x \le 1} \abs{f_3''(x)}  = 25.
\]
For $x \in [-1,1] \setminus [-0.6, 0.2]$ we note that $f_3''(x) = 0$. Thus, the definition of the cone is satisfied.
However, if $\fh$ is too large, say, $0.1 < h_\pm \le \fh$, and $x_\pm = -0.5\pm
h_\pm$, then $m(f_3,x_-,-0.5)=m(f_3,-0.5,x_+)=0$. Since $f_3''(-0.5) = 25$,
now $f_3 \notin \cc$. This example illustrates how $\fh$ determines the width of
a spiky function that may still lie in $\cc$.

\begin{figure}[t]
\centering
\includegraphics[width=5.7cm]{f3plot.eps} \quad
\includegraphics[width=5.7cm]{f3ppplot.eps}
\caption{The example $f_3$ with $-c=\delta = 0.2$  and its piecwise constant second derivative.}
\label{f3fig}
\end{figure}

The above examples of functions outside $\cc$ have discontinuities in the second
derivative.  If a function has sufficient smoothness and the higher order derivatives are nicely behaved in a certain sense, then we can be certain that this function lies in $\cc$.   If  $f \in C^3[a,b]$ and for all $x \in [a,b]$, it happens that
\begin{equation} \label{conesuffconda}
\norm[{[\min(a,x-h),\max(x+h,b)]}]{f'''} \le \frac{\abs{f''(x)}}{h} \left( 1 - \frac{1}{\fC(h)} \right) \ \ \forall h \in [0,\fh],
\end{equation}
then one must have $f \in \cc$.  Using a Taylor expansion it follows that.
\begin{align*}
m(f,\min(a,x-h),\max(x+h,b)) & = \inf \abs{f''(]\min(a,x-h),\max(x+h,b)[)} \\
& \ge \abs{f''(x)}  - \norm[{[\min(a,x-h),\max(x+h,b)]}]{f'''}h \\
& \ge \abs{f''(x)}  - \abs{f''(x)}\left( 1 - \frac{1}{\fC(h)} \right) \\
& \ge \frac{\abs{f''(x)}}{\fC(h)} \qquad \forall x \in [a,b],\ h \in [0,\fh].
\end{align*}
Thus, $f \in \cc$ by the definition of the cone in \eqref{conedef}.

Sufficient condition \eqref{conesuffconda} fails if $f''(x) = 0$ for some $x$.  For $x \in ]a + \fh, b-\fh[$ one may replace \eqref{conesuffconda} by an alternative sufficient condition if  $f \in C^4[a,b]$:
\begin{multline} \label{conesuffcondb}
\max_{0 \le s(t-x) \le h }{\abs{f''''(t)}} \le \frac{2\abs{f''(x)}}{h^2} \left( 1 - \frac{1}{\fC(h)} \right) +  \frac{2\abs{f'''(x)}}{h}  \\ \forall  h \in [0,\fh], \ s = \sign(f''(x)f'''(x)).
\end{multline}
Note that here $s$ depends on $x$.  For a particular $x$, suppose that $s = +1$.  Then it follows by a Taylor expansion that
\begin{align*}
m(f,x,x+h) & = \inf \abs{f''(]x,x+h[)} \\
& \ge \inf_{x < t < x+h} \biggl\{ \abs{f''(x)} + \abs{f'''(x)}(t-x)  - \frac{\norm[{[x,x+h]}]{f''''}(t-x)^2}{2} \biggr\}\\
& \ge \inf_{x < t < x+h} \biggl\{ \abs{f''(x)} \biggl[ \frac{1}{\fC(h)} + \left(1 - \frac{(t-x)^2}{h^2}\right)\left(1 - \frac{1}{\fC(h)}\right) \biggr] \\
& \qquad \qquad + \abs{f'''(x)}(t-x)\left(1 -  \frac{t-x}{h}\right)  \biggr\}\\
& \ge  \frac{\abs{f''(x)}}{\fC(h)} \qquad \forall x \in [a,b],\ h \in [0,\fh].
\end{align*}
Thus, $f \in \cc$ by the definition of the cone in \eqref{conedef}.  A similar argument establishes the case $s = -1$.
%--------------------------------------------------
\subsection{The Linear Spline and Its Error}
%--------------------------------------------------

Define a \emph{partition} of an interval $[a, b]$, denoted $\datasites$, to be
an ordered sequence of points that includes the endpoints of the interval,
$a=:x_0 < x_1 < \cdots < x_{n-1} < x_{n}:=b$, where $x_{i+1}-x_{i-2} <\fh, \forall i=2,\ldots, n-1$.  The linear spline
approximation to a function $f$ based on the partition $\datasites$ is denoted,
$S(f,\datasites)$ and defined as
\begin{multline} \label{splinedef}
S(f,\datasites)(x) =  \frac{x-x_i}{x_{i-1} - x_i} f(x_{i-1}) + \frac{x-x_{i-1}}{x_{i} - x_{i-1}}f(x_i), \\ x_{i-1} \le x \le x_i, \ i=1, \ldots, n.
\end{multline}
The error of this approximation is bounded as follows \cite{??}
\begin{equation} \label{appxerrbda}
\norm{f - S(f,\datasites)} \le \max_{i=1, \ldots, n} \frac{(x_i - x_{i-1})^2\norm[{[x_{i-1},x_i]}]{f''}}{8}.
\end{equation}
To construct an adaptive algorithm, one requires an upper bound on
$\norm[{[x_{i-1},x_i]}]{f''}$ in terms of function values. This can be done for
$f \in \cc$ using the correspondence between a second order difference and the
second derivative in~\eqref{minfppdef}.

For all $ x \in [x_{i-1},x_i]$,  denote
\begin{align*}
&h_- = x - x_{i-3}, \qquad x_- = x_{i-3},  \qquad i=3,4,\ldots,n,\\
 &h_+ = x_{i+2} - x, \qquad x_+ =  x_{i+2}, \qquad i=1,2,\ldots,n-2.
\end{align*}
Let
\begin{align}\label{bpf}
B_{+}(f,[x_{i-1},x_i])&=\begin{cases}
    \fC(x_{i+2}-x_{i-1})\abs{f[x_i,x_{i+1},x_{i+2}]},  & i=1,\ldots,n-2,
\\ 0, & i=n-1,n.
\end{cases}\\ 
\label{bmf}
 B_{-}(f,[x_{i-1},x_i])&=\begin{cases}
   0,  & i=1,2,
\\ \fC(x_{i}-x_{i-3})\abs{f[x_{i-3},x_{i-2},x_{i-1}])}, & i=3,\ldots,n.
\end{cases}
\end{align}
At first, we consider the case of an interior interval, i.e., $i \in \{3, \ldots, n-2\}$. Then it follows by the definition of the cone that
\begin{equation*}
\abs{f''(x)} \le \max\bigl(\fC(h_-)m(f,x_{i-3},x),\fC(h_+)m(f,x,x_{i+2})\bigr)  \quad  \forall x \in [x_{i-1},x_i].
\end{equation*}
Applying the fact that $\fC$ is non-decreasing and property \eqref{mdec} yields
the following upper bound for the norm of $f''$ in terms of function values:
\begin{align}\label{normbd}
\nonumber
\MoveEqLeft{\norm[{[x_{i-1},x_i]}]{f''}}\\
\nonumber
 \le  & \sup_{x_{i-1} \le x \le x_i} \bigl[\max\bigl(\fC(x-x_{i-3})m(f,x_{i-3},x),\fC(x_{i+2}-x)m(f,x,x_{i+2})\bigr)\bigr]  \\
 %\nonumber & \qquad \qquad \times \max\bigl(m(f,x_{i-3},x),m(f,x,x_{i+2})\bigr] \\
\nonumber
 \le  &  \max\bigl(\fC(x_{i}-x_{i-3})m(f,x_{i-3},x_{i-1}),\fC(x_{i+2}-x_{i-1})m(f,x_i,x_{i+2})\bigr) \\
\nonumber  \le & \max\bigl(\fC(x_{i}-x_{i-3})\abs{f[x_{i-3},x_{i-2},x_{i-1}])},\bigr.
 \fC(x_{i+2}-x_{i-1})\abs{f[x_i,x_{i+1},x_{i+2}]}\bigr) \\
 =  & \max\bigl(B_{\pm}(f,[x_{i-1},x_i])\bigr).
\end{align}

By a similar argument, we can obtain \eqref{normbd} for the sub-intervals on the left and right borders of the interval. Hence,
\eqref{normbd} is satisfied for all $i=1,\ldots,n$.
The bound in \eqref{normbd} combined with \eqref{appxerrbda} yield the
data-driven error bound for the linear spline:
\begin{equation} \label{appxerrbdb}
\norm{f - S(f,\datasites)} \le
\max_{i=1, \ldots, n} \frac{(x_i - x_{i-1})^2\max\bigl(B_{\pm}(f,[x_{i-1},x_i])\bigr)}{8} .
\end{equation}
The goal is to increase the number of nodes in the partition as needed to make
this error bound smaller than the desired tolerance.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Function Approximation Algorithm, $A$}\label{sec:fappx}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{The Algorithm}
Instead of defining the cone in terms of $\fh$ directly, we choose an initial number of points
\begin{equation}
\label{nodefinition}
n_{\ninit} = \max\left\{\left\lceil n_{\text{hi}}
\left(\frac{n_{\lo}}{n_{\text{hi}}}\right)^{\frac{1}{1+b-a}}\right\rceil ,5\right\}.
\end{equation}
that lies somewhere between $n_{\lo}$ and $n_{\text{hi}}$, depending on the width $[a,b]$.  Then, we choose $\fh := (b-a)/n_{\ninit}$.  In this way, $n_{\ninit} + 1$ becomes the minimum number of sub-intervals used by the algorithm.  A higher $n_{\ninit}$ corresponds to a more robust algorithm.  By the arguments of the previous section, the following algorithm solves function approximation problem \eqref{appxprob}.

\begin{algoA} \label{AlgoA}
For some finite interval, $[a,b]$, some fixed integers $n_{\lo}, n_{\text{hi}}$ satisfying $5 \le n_{\lo} \le n_{\text{hi}}$, and some non-decreasing
$\fC:[0,b-a] \to [1, \infty)$, let $f:[a,b] \to \reals$ and $\abstol >0$ be
user inputs. Choose $\fh := (b-a)/n_{\ninit}$, where and $n_{\ninit}$ is defined in \eqref{nodefinition}, and let  $n=n_{\ninit}-1$.  
Define the
following $n+1$ equally spaced points and the index set of intervals for which the error tolerance
is not yet satisified: 
$$x_i=a+\frac{i}{n}(b-a), \ i=0,\ldots,n, \qquad
\mathcal{I} = \{1,2,\ldots,n-1,n\}.$$ 
Choose the inflation function $\fC$. Compute the second-order divided difference, $f[x_{i-1},
x_{i}, x_{i+1}]$, for $i = \{1,2,\ldots,n-1\}$. Then do the
following:
\begin{enumerate}[\bf Step 1.]%\hspace{8.5ex}
%\renewcommand{\labelenumi}{\textbf{Step \arabic{enumi}.}}
\item \label{stage1} Check for convergence.
Compute $B_\pm(f,[x_{i-1},x_i])$ as in \eqref{bpf} and \eqref{bmf}, where $i \in \mathcal{I}$.
Let
\[
\mathcal{I}_\pm = \left\{i \in \mathcal{I}: \frac{(x_i - x_{i-1})^2B_\pm(f,[x_{i-1},x_i])}{8}  > \abstol \right\}.
\]
The update $\ci$ to be $\mathcal{I}_+ \cup \mathcal{I}_-$.  If $\mathcal{I} = \emptyset$, return the linear spline $A(f,\varepsilon) = S(f, \{x_i\}_{i=0}^n)$ and terminate the algorithm.
Otherwise, continue to the next step.
\item \label{stage2}
Denote $\widehat{\mathcal{I}}=\widehat{\mathcal{I}}_{+1} \cup \widehat{\mathcal{I}}_{+2} \cup \widehat{\mathcal{I}}_{-1} \cup \widehat{\mathcal{I}}_{-2},$ where
\begin{multline*}
\widehat{\mathcal{I}}_{\pm k} = \left\{i \in \{1,2,\ldots,n_0\}: \frac{(x_j - x_{j-1})^2B_\pm(f,[x_{j-1},x_j])}{8}  > \abstol,\right.\\
 \left.j=i\mp k, 0<j\le n_0, k=1,2. \right\},
\end{multline*}
\[\widetilde{\mathcal{I}}=\left\{i \in \mathcal{I} \cup \widehat{\mathcal{I}}: x_i - x_{i-1}=\max\limits_{j \in \mathcal{I} \cup \widehat{\mathcal{I}} } x_j-x_{j-1} \right\}.\]
Split in half those intervals $[x_{i-1},x_i]$ for which $i$ lies in $\widetilde{\mathcal{I}}$.
Update $n$, $\{x_i\}_{i=0}^n$, $\mathcal{I}$, and the $f[x_{i-1}, x_{i}, x_{i+1}]$ accordingly.  Return to Step~\ref{stage1}.
\end{enumerate}
\end{algoA}

\begin{theorem}
Algorithm $A$ defined above solves problem \eqref{appxprob} for functions in the cone $\cc$ defined in \eqref{conedef}.
\end{theorem}

\input{localcost}

\xinnote{
we need a theorem that works.\\
Numerical examples.\\
We need computational cost\\
complexity bound
}



\section{The Minimization Algorithm, $M$}
\xinnote{
we need a theorem that works.\\
Numerical examples.\\
We need computational cost\\
complexity bound
}
\section{Discussion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{References}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliography{FJH23,FJHOwn23}

\end{document}








