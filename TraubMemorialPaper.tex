\documentclass[review]{elsarticle}
%\documentclass[final]{elsarticle}

\usepackage{amsmath,amssymb,amsthm,xspace,mathtools,hyperref,color}
\usepackage{mathrsfs,verbatim,xspace}

\allowdisplaybreaks
\input{FJHDef.tex}

\journal{Journal of Complexity}

%%%%%%%%%%%%%%%%%%%%%%%
%% Elsevier bibliography styles
%%%%%%%%%%%%%%%%%%%%%%%
%% To change the style, put a % in front of the second line of the current style and
%% remove the % from the second line of the style you would like to use.
%%%%%%%%%%%%%%%%%%%%%%%

%% Numbered
%\bibliographystyle{model1-num-names}

%% Numbered without titles
%\bibliographystyle{model1a-num-names}

%% Harvard
%\bibliographystyle{model2-names.bst}\biboptions{authoryear}

%% Vancouver numbered
%\usepackage{numcompress}\bibliographystyle{model3-num-names}

%% Vancouver name/year
\usepackage{numcompress}\bibliographystyle{model4-names}\biboptions{authoryear}

%% APA style
%\bibliographystyle{model5-names}\biboptions{authoryear}

%% AMA style
%\usepackage{numcompress}\bibliographystyle{model6-num-names}

%% `Elsevier LaTeX' style
%\bibliographystyle{elsarticle-num}
%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\abstol}{\varepsilon_{\textrm{a}}}
\newcommand{\datasites}{\{x_i\}_{i=0}^n}
\makeatletter
\newcommand{\vast}{\bBigg@{4}}
\newcommand{\Vast}{\bBigg@{6}}
\makeatother
%\newdefinition{algo}{Algorithm}
\theoremstyle{definition}
\newtheorem*{algoA}{Algorithm $A$}
\newtheorem*{algoM}{Algorithm $M$}
\newcommand{\vastl}{\mathopen\vast}
\newcommand{\vastm}{\mathrel\vast}
\newcommand{\vastr}{\mathclose\vast}
\newcommand{\Vastl}{\mathopen\Vast}
\newcommand{\Vastm}{\mathrel\Vast}
\newcommand{\Vastr}{\mathclose\Vast}

\definecolor{orange}{rgb}{1.0,0.3,0.0}
\definecolor{violet}{rgb}{0.75,0,1}
\definecolor{darkgreen}{rgb}{0,0.6,0}
\newcommand{\frednote}[1]{  {\textcolor{red}  {\mbox{**Fred:} #1}}}
\newcommand{\yuhannote}[1]{  {\textcolor{darkgreen}  {\mbox{**Yuhan:} #1}}}
\newcommand{\xinnote}[1]{ {\textcolor{violet}  {\mbox{**Xin:} #1}}}
\newcommand{\scnote}[1]{ {\textcolor{orange}  {\mbox{**SC:} #1}}}

\newcommand{\Ixl}{I_{x,l}}
\newcommand{\Ixlx}{I_{x,\ell(x)}}
\newcommand{\Ixrlx}{I_{x,\rell(x)}}
\newcommand{\Ixhlx}{I_{x,\hell(x)}}
\newcommand{\hell}{\hat{\ell}}
\newcommand{\tell}{\tilde{\ell}}
\newcommand{\rell}{\mathring{\ell}}
\newcommand{\tgamma}{\widetilde{\gamma}}
\DeclareMathOperator{\lo}{lo}
\DeclareMathOperator{\ninit}{ninit}
\DeclareMathOperator{\errest}{errest}
\newtheorem{theorem}{Theorem}
\newtheorem{exmp}{Example}
\newtheorem{prop}[theorem]{Proposition}
\newcommand{\funappxg}{\texttt{funappx\_g\xspace}}
\newcommand{\funappxglobalg}{\texttt{funappxglobal\_g\xspace}}
\newcommand{\funming}{\texttt{funmin\_g\xspace}}
\newcommand{\integralg}{\texttt{integral\_g\xspace}}
\newcommand{\cosappx}{\operatorname*{cosappx}}
\newcommand{\sinappx}{\operatorname*{sinappx}}
\newcommand{\minfi}{\min\limits_{1\le i \le n} f(x_i)} %minf(xi)
\newcommand{\minfii}{\min\{f(x_{i-1}, f(x_i)\}} %min{f(x_i-1),f(x_i)}
\begin{document}

\begin{frontmatter}

\title{Local Adpation for Approximation and Optimization of Univariate Functions}


%% Group authors per affiliation:
\author{Sou-Cheng T.~Choi}
\author{Yuhan Ding}
\author{Fred J.~Hickernell}
\author{Xin Tong}
\address{Department of Applied Mathematics, Illinois Institute of Technology, RE 208, 10 West 32$^{\text{nd}}$ Street, Chicago, Illinois, 60616, USA}

\begin{abstract}
Some ideas to get us going
\end{abstract}

\begin{keyword}
\sep \sep
\MSC[2010]  \sep
\end{keyword}

\end{frontmatter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction} \label{sec:intro}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The goal is to solve  function approximation and optimization problems by locally adaptive algorithms. For some suitable set, $\cc$, of continuous,
real-valued functions on a finite interval $[a,b]$, we  construct
algorithms $A:(\cc,(0,\infty)) \to L^{\infty}[a,b]$ and $M: (\cc,(0,\infty)) \to
\reals$ such that for any $f \in \cc$ and any error tolerance $\abstol > 0$,
\begin{gather}
\norm{f - A(f,\abstol)} \le \abstol,  \tag{APP} \label{appxprob} \\
M(f,\abstol) - \min_{a \le x \le b} f(x)  \le \abstol. \tag{MIN} \label{optprob}
\end{gather}
Here, $\norm{\cdot}$ denotes the $L^{\infty}$-norm.  The algorithms $A$ and $M$ depend only on function values. These algorithms choose their data sites in $[a,b]$ adaptively, with each new site depending on the function data already obtained.  Each algorithm  automatically determines when to stop sampling
and return the correct answer.  These algorithms sample more densely where needed, i.e., they are locally adaptive.

Adaptive algorithms relieve the user of having to specify the number of samples required.  The user only needs to provide the error tolerance and be satisfied with the parameters that define $\cc$ and thereby specify the robustness of the algorithms.

We accomplish the following:
\begin{itemize}

\item The set $\cc$ is defined in Section~\ref{sec:conedef} to be a cone contained in $\cw^{2,\infty}$, the Sobolev space of functions whose second derivatives have finite sup-norms.  Because any algorithm can be fooled by a function that is sufficiently spiky, this $\cc$ is defined to exclude functions that are too spiky.  We construct a data-based upper bound on $\norm[{[\alpha,\beta]}]{f''}$ in~\eqref{normbd} in terms of second-order divided differences and this provides a data-based upper bound on the error of the linear spline in~\eqref{appxerrbdb}.

\item Algorithms $A$ and $M$ are constructed in Sections~\ref{subsec:appxalgo} and~\ref{sec:minalgo}, which solve problems~\eqref{appxprob}
and~\eqref{optprob}, respectively.  These algorithms are based on linear splines.  Guarantees of success are provided by Theorems~\ref{thm:algAworks}
and~\ref{thm:algMworks}.

\item Upper bounds on the computational costs of these algorithms  are derived in Sections~\ref{subsec:appxcost} and~\ref{subsec:optcost}.  These upper bounds in Theorems~\ref{thm:cost} and~\ref{??} are roughly proportional to $\sqrt{\norm[\frac12]{f''}/\abstol}$ (see~\eqref{costbdapprox}).  Here, $\norm[\frac12]{\cdot}$ denotes the $L^{\frac12}$-norm.  Since $\norm[\frac12]{f''}$ can be much smaller than $\norm[\frac12]{\cdot}$ (see
Proposition~\ref{equivnormprop}), it becomes apparent how much more efficient locally adaptive algorithms can be than globally adaptive algorithms with their computational costs  proportional to $\sqrt{\norm{f''}/\abstol}$.

\item Lower bounds on the computational complexity of problems~\eqref{appxprob} and \eqref{optprob} are derived in Sections~\ref{subsec:appxcomp}
and~\ref{subsec:optcomp}.  The lower bounds in Theorems~\ref{thm:A_cost} and~\ref{??} are also proportional to $\sqrt{\norm[\frac12]{f''}/\abstol}$, which makes our algorithms essentially asymptotically optimal.

\end{itemize}

\cite{Nov96a} summarizes the settings under which adaption may provide an advantage over nonadaption.  For linear problems, such as~\eqref{appxprob}, adaption has no advantage if the set of functions being considered is symmetric and convex \cite[Chapter 4, Theorem 5.2.1]{TraWasWoz88} \cite[Theorem 1]{Nov96a}.  The $\cc$ defined for our approximation algorithm $A$ is symmetric, but not convex.  \cite{PlaEtal08a} have developed adaptive algorithms for functions with singularities.  Our algorithms are not designed for functions with singularities.

This work emulates the approach taken by \cite{HicEtal14b} to develop globally adaptive algorithms for integration and function approximation.  ``Globally" means that the sampling density is constant but that the number of samples is chosen adaptively.  \cite{HicEtal14b} relies on linear splines applied to cones of functions that are not too spiky, as we do here.  \cite{Ton14a} extends these ideas to optimization, and \cite{Din15a} develops a locally adaptive function approximation algorithm.  Here we build upon the ideas of \cite{Din15a} and apply to a somewhat different cone of functions to obtain algorithms whose cost does not increase drastically as they are made more robust to spiky input functions.  There is also a parallel development of adaptive algorithms for multivariate integration via Monte Carlo methods \citep{HicEtal14a, Jia16a} and quasi-Monte Carlo methods \citep{HicJim16a,JimHic16a}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Cone, $\cc$, of Functions of Interest} \label{sec:cone}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Linear splines are the foundation for adaptive algorithms $A$ and $M$. The best linear spline error possible occurs for the Sobolev space of functions whose second derivatives have finite $L^{\infty}$-norm:
\[
\cw^{2,\infty} := \Bigl \{f \in C^1[a,b] : \norm{f''} : = \norm[{[a,b]}]{f''} : = \sup_{a \le x \le b} \abs{f''(x)} <  \infty \Bigr\}.
\]

To bound the error of the linear spline, our adaptive algorithms construct data-based upper bounds on $\norm[{[\alpha,
\beta]}]{f''}$ in terms of function values.  Such bounds are given in~\eqref{normbd} below.  These data-based bounds arise from Newton's divided differences, introduced in the next section.  For these bounds to hold, we must  assume that $f''(x)$ does not change drastically with a small change in $x$.  These assumptions define our cone, $\cc$, of functions for which our algorithms ultimately apply.  This cone is defined in Section~\ref{sec:conedef} in~\eqref{conedef}.

%--------------------------------------------------
\subsection{Newton's Divided Differences} \label{sec:ndd}
%--------------------------------------------------

Functions in $\cw^{2,\infty}$ may have $f''(x)$ undefined for countably many $x \in [a,b]$.  Thus, we define $f''(x)$ as an interval-valued function:
\begin{gather*}
f''(x) := \Bigl[\liminf_{t \to x} f''(t), \ \limsup_{t \to x} f''(t) \Bigr], \\
 f''(]\alpha, \beta[) := \bigcup_ {x \in ]\alpha, \beta[} f''(x), \qquad  ]\alpha, \beta[ \subset [a,b],
\end{gather*}
where $]\alpha, \beta[$ denotes the open interval  $\{x:\alpha<x<\beta\}$.
We define a
measure of how small $\abs{f''(x)}$ can be for  $x \in ]\alpha, \beta[$ as follows:
\begin{equation} \label{minfppdef}
m(f,\alpha, \beta) = \inf \abs{f''(]\alpha, \beta[)}.
\end{equation}
It follows from this definition
that
\begin{equation} \label{mdec}
m(f,\alpha,\beta) \le m(f,\gamma,\delta) \qquad \forall a \le \alpha \le \gamma \le \delta \le \beta \le b.
\end{equation}

We cannot determine $f''(x)$ for a specific $x$ based only on values of $f$. However,
we can use values of $f$ to provide an upper bound on $m(f,\alpha, \beta)$ via Newton divided differences.

Let $p$ denote the Lagrange quadratic interpolating polynomial at the nodes
$\{x_1, x_2, x_3\}$, which may be written as
\begin{equation*}
p(x) : = f[x_1] + f[x_1, x_2](x-x_1) + f[x_1, x_2, x_3](x-x_1)(x-x_2),
\end{equation*}
where the $f[\cdots]$ are the Newton divided differences; see for example~\cite{CheKin12a}. In particular,
\begin{gather}
\nonumber
f[x_1] = f(x_1), \qquad f[x_1, x_2] = \frac{f[x_2] - f[x_1]}{x_2-x_1},  \\
f[x_1, x_2,x_3] = \frac{f[x_2,x_3] - f[x_1,x_2]}{x_3-x_1}. \label{divdiff}
\end{gather}
For any $f \in
\cw^{2,\infty}$, the function $f - p$ has at least three distinct zeros on
$[x_1, x_3]$, so $f' - p'$ has at least two distinct zeros on $]x_1, x_3[$,
i.e., there exists $\xi_\pm$ with $x_1 < \xi_- < x_2 < \xi_+ < x_3$ with
$f'(\xi_\pm) - p'(\xi_{\pm}) = 0$. If $f''$ is continuous, then we can conclude
that $ f[x_1, x_2, x_3]= p''(\zeta) =f''(\zeta) $ for some $\zeta \in ]x_1,
x_3[$. However, $\cw^{2,\infty}$ contains functions without continuous
second derivatives. Fortunately, we can obtain a somewhat weaker---yet equally useful result---by  definition~\eqref{minfppdef}:
\begin{multline*}
\bigabs{f[x_1, x_2, x_3]}(\xi_+  - \xi_-) = \abs{p'(\xi_+) - p'(\xi_-)} =  \abs{f'(\xi_+) - f'(\xi_-)} \\
= \abs{\int_{\xi_-}^{\xi_+} f''(x) \, \dif x} \ge m(f,\xi_-, \xi_+) (\xi_+  - \xi_-)  \ge m(f,x_1, x_3) (\xi_+  - \xi_-) .
\end{multline*}
So the the data-based second-order divided difference provides an upper
bound on how small $\abs{f''}$ can be in the interval $]x_1, x_3[$:
\begin{equation} \label{NDDbdm}
\bigabs{f[x_1, x_2, x_3]} \ge m(f,x_1, x_3).
\end{equation}

%--------------------------------------------------
\subsection{The Cone Definition}  \label{sec:conedef}
%--------------------------------------------------

Let $\fh$ be some positive number, and let $\fC : [0,\fh[ \to [1,\infty[$ be any
non-decreasing function, which serves in our subsequent algorithms as inflation factors dependent on the grid size.
These parameters are used to define a cone of functions for which our algorithms will
apply.  This cone only includes functions whose second derivatives do not change much in
magnitude over a short distance.  Thus, they are \emph{not too spiky}.  Specifically,
\begin{multline} \label{conedef}
 \cc :=   \Bigl \{
 f  \in    \cw^{2,\infty}:   \max\abs{f''(x)}  \le \tB(f,x,h_-,h_+)  \text{ for all } x \in [a,b],
\\ \text{with }  \max(h_{\pm}) \in ]0, \fh[  \Bigr \},
\end{multline}
where the pointwise upper bound, $\tB(f,x,h_-,h_+)$, is defined in terms of $x_{\pm} =x \pm h_{\pm}$ as follows:
\begin{multline*}
\tB(f,x,h_-,h_+)=\\
\begin{cases}
  \max\bigl(\fC(h_{-}) m(f,x_-,x),\fC(h_{+}) m(f,x,x_+)\bigr), & a \le x_- \le x_+ \le b,\\
\fC(h_{-}) m(f,x_-,x), & a \le x_- \le b <  x_+,\\
\fC(h_{+}) m(f,x,x_+), & x_- < a \le x_+ \le b.
\end{cases} %\quad
 %\Vastr
\end{multline*}
Either decreasing $\fh$ or increasing the function $\fC$ expands the cone to include more functions.  An example of $\fC$ is
\begin{equation} \label{sampleC}
\fC(h) : = \frac{\fC(0) \fh}{\fh - h}, \quad h \in [0,\fh[, \qquad \fC(0) > 1,\ \fh \le b - a.
\end{equation}

This $\cc$ is a cone because $f \in \cc \implies cf \in \cc$ for all real
$c$. Cones of
functions are key to the theoretically justified adaptive algorithms in
\cite{HicEtal14b}, \cite{Ton14a}, and \cite{Din15a}.

A function $f$ with $f''(\alpha) = f''(\beta) = \{0\} \ne f''((\alpha+\beta)/2)$ may
lie inside $\cc$ only if $\beta - \alpha > 2\fh$. Thus, $f''$ cannot have zeros too close to each other.  Except near the endpoints of
the interval, the definition of $\cc$ uses values of $f''$ on both sides of $x$
to bound $\max \abs{f''(x)}$. This allows $\cc$ to include functions with step
discontinuities in their second derivatives, provided that these discontinuities
do not occur too close to each other or too close to the ends of the interval.

We provide examples of functions lying outside $\cc$
and similar functions lying inside $\cc$. Consider these two functions defined
on $[-1,1]$ whose second derivatives oscillate wildly near $0$:
\begin{gather*}
f_1(x) = x^4 \sin(1/x), \\
 f_1''(x) = \begin{cases} (12x^2 - 1) \sin(1/x) -6 x \cos(1/x), & x \ne 0 \\
 [-1,1], & x = 0, \end{cases} \\
f_2(x) = 10  x^2 + f_1(x), \qquad f_2''(x) = 20+ f_1''(x).
\end{gather*}
These functions are plotted in Figure~\ref{f1f2fig}. Because the $f''_1(x)$
takes on both signs for $x$ arbitrarily close to $0$ and on either side of $0$,
it follows that  $m(f_1,-h_-,0) = m(f_1,0,h_+) = 0$ for all $h_\pm \in [0,1]$.
However, $\max\abs{f''_1(0)} = 1$, so $f_1$ cannot lie inside
$\cc$ no matter how $\fh$ and $\fC$ are defined. On the other hand,
$m(f_2,\alpha, \beta) \ge 13.5$ for all $-1 \le \alpha < \beta \le 1$, and
$\max \abs{f''_2(x)} \le 27$ for all $x \in [-1,1]$, so $f_2 \in
\cc$ if $\fC(0) \ge 2$.

\begin{figure}[t]
\centering
\includegraphics[width=5.6cm]{figure/f1f2plot.eps} \quad
\includegraphics[width=5.9cm]{figure/f1closeplot.eps} \\
\includegraphics[width=5.6cm]{figure/f1ppf2ppplot.eps} \quad
\includegraphics[width=5.9cm]{figure/f2closeplot.eps}
\caption{The examples $f_1$ and $f_2$ and their second derivatives. Note that
$f_2''(x) = f_1''(x) + 20$. \label{f1f2fig}}
\end{figure}

Consider the following function defined on $[-1,1]$, whose second derivative has jump discontinuities:
\begin{align} \label{f3def}
f_3(x) & = \begin{cases} \displaystyle
   \frac{1}{2\delta^2} \Bigl [4 \delta^2 + (x-c)^2 + (x-c-\delta)\abs{x-c-\delta}
\\ \qquad \qquad
    - (x-c+\delta)\abs{x-c+\delta} \Bigr ], & \abs{x-c} \le 2\delta,
\\ 0, & \text{otherwise},
\end{cases} \\
\nonumber
f''_3(x) & =
\begin{cases} \displaystyle
    \frac{1}{\delta^2} [1 + \sign(x-c-\delta) - \sign(x-c+\delta), & \abs{x-c} \le 2\delta,
\\ 0, & \text{otherwise}.
\end{cases}
\end{align}
Here $c$ and $\delta$ are parameters satisfying $-1 \le c-2 \delta < c+ 2\delta \le 1$. This function and its second derivative
are shown in Figure~\ref{f3fig} for $-c=\delta = 0.2$.

If $\delta \ge 2 \fh$, then $f_3 \in \cc$ for
any choice of $\fC : [0,b-a] \to [1,\infty)$.  We see this by examining two cases.  For all $x \in [c - 2 \delta, c + 2 \delta]$ and all $h \in \, ]0,\fh[ \, \subseteq \, ]0,\delta/2[$, let $x_- = \max(a, x -h)$ and $x_+ = \min(x +h,b)$.  Then
\[
m(f_3,x_-,x) = m(f_3,x,x_+) = \sup_{-1 \le x \le 1} \abs{f_3''(x)}  = \delta^{-2}.
\]
For $x \in [-1,1] \setminus [c - 2 \delta , c + 2 \delta]$ we note that $f_3''(x) = 0$. Thus, the definition of the cone is satisfied.

However, if $\delta < 2 \fh$, then for $x = c-3\delta/2$ and $\delta/2 < h < \fh$, then regardless of how $\fC$ is defined,
\[
\fC(h)m(f_3,x - h,x)=\fC(h)m(f_3,x,x+h)=0 < \abs{f''_3(x)} = \delta^{-2},
\]
which violates the definition of $\cc$.  For $\delta < 2 \fh$ the function $f_3$ is too spiky to lie in the cone $\cc$.   This example illustrates how the choice of $\fh$ influences the width of
a spiky function that may or may not lie in $\cc$.

\begin{figure}[t]
\centering
\includegraphics[width=5.7cm]{figure/f3plot.eps} \quad
\includegraphics[width=5.7cm]{figure/f3ppplot.eps}
\caption{The example $f_3$ with $-c=\delta = 0.2$  and its piecwise constant second derivative. \label{f3fig}}
\end{figure}

The above examples of functions outside $\cc$ have discontinuities in the second
derivative.  If a function has sufficient smoothness and the higher order derivatives are nicely behaved in a certain sense, then we can be sure that this function lies in $\cc$.   If  $f \in C^3[a,b]$ and for all $x \in [a,b]$, it happens that
\begin{equation} \label{conesuffconda}
\norm[{[\min(a,x-h),\max(x+h,b)]}]{f'''} \le \frac{\abs{f''(x)}}{h} \left( 1 - \frac{1}{\fC(h)} \right) \ \ \forall h \in [0,\fh],
\end{equation}
then one must have $f \in \cc$.  Using a Taylor expansion it follows that.
\begin{align*}
m(f,\min(a,x-h),\max(x+h,b)) & = \inf \abs{f''(]\min(a,x-h),\max(x+h,b)[)} \\
& \ge \abs{f''(x)}  - \norm[{[\min(a,x-h),\max(x+h,b)]}]{f'''}h \\
& \ge \abs{f''(x)}  - \abs{f''(x)}\left( 1 - \frac{1}{\fC(h)} \right) \\
& \ge \frac{\abs{f''(x)}}{\fC(h)} \qquad \forall x \in [a,b],\ h \in [0,\fh].
\end{align*}
Thus, $f \in \cc$ by the definition of the cone in~\eqref{conedef}.

Sufficient condition~\eqref{conesuffconda} fails if $f''(x) = 0$ for some $x$.  For $x \in ]a + \fh, b-\fh[$ one may replace~\eqref{conesuffconda} by an alternative sufficient condition if  $f \in C^4[a,b]$:
\begin{multline} \label{conesuffcondb}
\max_{0 \le s(t-x) \le h }{\abs{f''''(t)}} \le \frac{2\abs{f''(x)}}{h^2} \left( 1 - \frac{1}{\fC(h)} \right) +  \frac{2\abs{f'''(x)}}{h}  \\ \forall  h \in [0,\fh], \ s = \sign(f''(x)f'''(x)).
\end{multline}
Note that here $s$ depends on $x$.  For a particular $x$, suppose that $s = +1$.  Then it follows by a Taylor expansion that
\begin{align*}
m(f,x,x+h) & = \inf \abs{f''(]x,x+h[)} \\
& \ge \inf_{x < t < x+h} \biggl\{ \abs{f''(x)} + \abs{f'''(x)}(t-x)  - \frac{\norm[{[x,x+h]}]{f''''}(t-x)^2}{2} \biggr\}\\
& \ge \inf_{x < t < x+h} \biggl\{ \abs{f''(x)} \biggl[ \frac{1}{\fC(h)} + \left(1 - \frac{(t-x)^2}{h^2}\right)\left(1 - \frac{1}{\fC(h)}\right) \biggr] \\
& \qquad \qquad + \abs{f'''(x)}(t-x)\left(1 -  \frac{t-x}{h}\right)  \biggr\}\\
& \ge  \frac{\abs{f''(x)}}{\fC(h)} \qquad \forall x \in [a,b],\ h \in [0,\fh].
\end{align*}
Thus, $f \in \cc$ by the definition of the cone in~\eqref{conedef}.  A similar argument establishes the case $s = -1$.

%--------------------------------------------------
\subsection{The Linear Spline and Its Error} \label{subsec:spline}
%--------------------------------------------------

Define a \emph{partition} of an interval $[a, b]$, denoted $\datasites$, to be
an ordered sequence of points that includes the endpoints of the interval,
$a=:x_0 < x_1 < \cdots < x_{n-1} < x_{n}:=b$, where $x_{i+1}-x_{i-2} <\fh, \forall i=2,\ldots, n-1$.  The linear spline
approximation to a function $f$ based on the partition $\datasites$ is denoted,
$S(f,\datasites)$ and defined as
\begin{multline} \label{splinedef}
S(f,\datasites)(x) =  \frac{x-x_i}{x_{i-1} - x_i} f(x_{i-1}) + \frac{x-x_{i-1}}{x_{i} - x_{i-1}}f(x_i), \\ x_{i-1} \le x \le x_i, \ i=1, \ldots, n.
\end{multline}
The error of this approximation is bounded as follows \cite{??}
\begin{equation} \label{appxerrbda}
\norm{f - S(f,\datasites)} \le \max_{i=1, \ldots, n} \frac{(x_i - x_{i-1})^2\norm[{[x_{i-1},x_i]}]{f''}}{8}.
\end{equation}
To construct an adaptive algorithm, one requires an upper bound on
$\norm[{[x_{i-1},x_i]}]{f''}$ in terms of function values. This can be done for
$f \in \cc$ using the correspondence between a second order difference and the
second derivative in~\eqref{minfppdef}.

For all $ x \in [x_{i-1},x_i]$,  denote
\begin{align*}
&h_- = x - x_{i-3}, \qquad x_- = x_{i-3},  \qquad i=3,4,\ldots,n,\\
 &h_+ = x_{i+2} - x, \qquad x_+ =  x_{i+2}, \qquad i=1,2,\ldots,n-2.
\end{align*}
Let $\cp$ be a shorthand notation for the partition $\{x_i\}_{i=1}^\infty$, and define the following data based quantities.
\begin{align}\label{bpf}
B_{i+}(f,\cp)&:=\begin{cases}
    \fC(x_{i+2}-x_{i-1})\abs{f[x_i,x_{i+1},x_{i+2}]},  & i=1,\ldots,n-2,
\\ 0, & i=n-1,n.
\end{cases} \\
\label{bmf}
 B_{i-}(f,\cp)&: =\begin{cases}
   0,  & i=1,2,
\\ \fC(x_{i}-x_{i-3})\abs{f[x_{i-3},x_{i-2},x_{i-1}])}, & i=3,\ldots,n,
\end{cases} \\
\nonumber
B_i(f,\cp) & : = \max\bigl(B_{i,\pm}(f,\cp) \bigr).
\end{align}
We now show that these give us a data-based upper bound on the norm of $f''$, namely,
\begin{equation}\label{normbd}
\norm[{[x_{i-1},x_i]}]{f''} \le B_i(f,\cp), \qquad i =1, \ldots, n.
\end{equation}

First, we consider the case of an interior interval, i.e., $i \in \{3, \ldots, n-2\}$. The definition of the cone implies that
\begin{equation*}
\abs{f''(x)} \le \max\bigl(\fC(h_-)m(f,x_{i-3},x),\fC(h_+)m(f,x,x_{i+2})\bigr)  \quad  \forall x \in [x_{i-1},x_i].
\end{equation*}
Applying the fact that $\fC$ is non-decreasing and property~\eqref{mdec} yields
\begin{align*}
\nonumber
\MoveEqLeft{\norm[{[x_{i-1},x_i]}]{f''}}\\
\nonumber
 \le  & \sup_{x_{i-1} \le x \le x_i} \bigl[\max\bigl(\fC(x-x_{i-3})m(f,x_{i-3},x),\fC(x_{i+2}-x)m(f,x,x_{i+2})\bigr)\bigr]  \\
 %\nonumber & \qquad \qquad \times \max\bigl(m(f,x_{i-3},x),m(f,x,x_{i+2})\bigr] \\
\nonumber
 \le  &  \max\bigl(\fC(x_{i}-x_{i-3})m(f,x_{i-3},x_{i-1}),\fC(x_{i+2}-x_{i-1})m(f,x_i,x_{i+2})\bigr) \\
\nonumber  \le & \max\bigl(\fC(x_{i}-x_{i-3})\abs{f[x_{i-3},x_{i-2},x_{i-1}])},\bigr.
 \fC(x_{i+2}-x_{i-1})\abs{f[x_i,x_{i+1},x_{i+2}]}\bigr) \\
 =  & \max\bigl(B_{i,\pm}(f,\cp)\bigr) = B_i(f,\cp).
\end{align*}
A similar argument applied for the sub-intervals on the left and right borders of the interval.
The bound in~\eqref{normbd} combined with~\eqref{appxerrbda} yield the
data-driven error bound for the linear spline:
\begin{equation} \label{appxerrbdb}
\norm{f - S(f,\datasites)} \le
\max_{i=1, \ldots, n} \frac{(x_i - x_{i-1})^2 B_{i}(f,\cp)}{8} .
\end{equation}
The goal is to increase the number of nodes in the partition as needed to make
this error bound smaller than the desired tolerance.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Function Approximation Algorithm, $A$}\label{sec:fappx}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Algorithm $A$} \label{subsec:appxalgo}
Instead of defining the cone in terms of $\fh$ directly, we choose an initial number of points depending on the width $[a,b]$:
\begin{equation}
\label{nodefinition}
n_{\ninit} = \left\lceil n_{\text{hi}}
\left(\frac{n_{\lo}}{n_{\text{hi}}}\right)^{\frac{1}{1+b-a}}\right\rceil, \qquad \fh := \frac{3(b-a)}{n_{\ninit}-2},
\end{equation}
where $n_{\lo}$ and $n_{\text{hi}}$ are integers satisfying $5 \le n_{\lo} \le n_{\text{hi}}$.  In this way, $n_{\ninit} -1$ becomes the minimum number of sub-intervals used by the algorithm.  A higher $n_{\ninit}$ corresponds to a more robust algorithm.  By the arguments of the previous section, the following algorithm solves function approximation problem~\eqref{appxprob}.

\begin{algoA} \label{AlgoA}
For some finite interval, $[a,b]$, some fixed integers $n_{\lo}, n_{\text{hi}}$ satisfying $5 \le n_{\lo} \le n_{\text{hi}}$, and some non-decreasing
$\fC:[0,b-a] \to [1, \infty)$, let $f:[a,b] \to \reals$ and $\abstol >0$ be
user inputs. Choose $n_{\ninit}$ and $\fh$ defined in~\eqref{nodefinition}, and let  $n=n_{\ninit}-1$.
Define the partition, $\cp$ of  equally spaced points and the index set of intervals for which the error tolerance
is not yet satisfied:
$$x_i=a+\frac{i}{n}(b-a), \ i=0,\ldots,n, \qquad
\mathcal{I} = \{1,2,\ldots,n-1,n\}.$$
Choose the inflation function $\fC$. Compute the second-order divided difference, $f[x_{i-1},
x_{i}, x_{i+1}]$, for $i = \{1,2,\ldots,n-1\}$. Then do the
following:
\begin{enumerate}[\bf Step 1.]%\hspace{8.5ex}
%\renewcommand{\labelenumi}{\textbf{Step \arabic{enumi}.}}
\item \label{stage1} Check for convergence.
Compute $B_{i,\pm}(f,\cp)$ as in~\eqref{bpf} and~\eqref{bmf}, where $i \in \mathcal{I}$.
Let
\[
\mathcal{I}_\pm = \left\{i \in \mathcal{I}: \frac{(x_i - x_{i-1})^2B_{i,\pm}(f,\cp)}{8}  > \abstol \right\}.
\]
Then update $\ci$ to be $\mathcal{I}_+ \cup \mathcal{I}_-$.  If $\mathcal{I} = \emptyset$, return the linear spline $A(f,\abstol) = S(f, \{x_i\}_{i=0}^n)$ and terminate the algorithm.
Otherwise, continue to the next step.
\item \label{stage2}
Denote $\widehat{\mathcal{I}}=\widehat{\mathcal{I}}_{+1} \cup \widehat{\mathcal{I}}_{+2} \cup \widehat{\mathcal{I}}_{-1} \cup \widehat{\mathcal{I}}_{-2},$ where
\begin{multline*}
\widehat{\mathcal{I}}_{\pm k} = \left\{i \in \{1,2,\ldots,n\}: \frac{(x_j - x_{j-1})^2B_{j,\pm}(f,\cp)}{8}  > \abstol,\right.\\
 \left.j=i\mp k, 0<j\le n, k=1,2. \right\},
\end{multline*}
\[\widetilde{\mathcal{I}}=\left\{i \in \mathcal{I} \cup \widehat{\mathcal{I}}: x_i - x_{i-1}=\max\limits_{j \in \mathcal{I} \cup \widehat{\mathcal{I}} } x_j-x_{j-1} \right\}.\]
Split in half those intervals $[x_{i-1},x_i]$ for which $i$ lies in $\widetilde{\mathcal{I}}$.
Update $n$, $\cp = \{x_i\}_{i=0}^n$, $\mathcal{I}$, and the $f[x_{i-1}, x_{i}, x_{i+1}]$ accordingly.  Return to Step~\ref{stage1}.
\end{enumerate}
\end{algoA}

\begin{theorem} \label{thm:algAworks}
Algorithm $A$ defined above solves problem~\eqref{appxprob} for functions in the cone $\cc$ defined in~\eqref{conedef}.
\end{theorem}

\subsection{The Computational Cost of $A$} \label{subsec:appxcost}

Next, we investigate the computational cost of our locally adaptive algorithm. Let $n_0= n_{\ninit} -1$ denote the initial number of intervals, and
 \[
 h_0=\frac{b-a}{n_0} = \frac{b-a}{n_{\ninit}-1} = \frac{(b-a)\fh}{3(b-a)+\fh}
 \]
be the initial width of the subintervals. To faciliate our derivation of the computational cost of $A$, we introduce the new notation $\Ixl$, which is the unique half-open interval containing $x$ with
 the width $2^{-l}h_0$.
\[\Ixl :=\left[a+(j-1)2^{-l}h_0,a+j \ 2^{-l}h_0\right[, \quad j=\left\lceil\frac{(x-a)2^l}{h_0}\right\rceil, \ l \in \mathbb{N}_0, \ x \in [a,b[.\]
Let
$\ell(x)$ be defined such that
$I_{x,\ell(x)}$ is the final subinterval in algorithm $A$ that contains $x$ when the algorithm terminates.
Let $\bar{I}_{x,l}$ be a similar closed interval with generally five times the width:
\[\bar{I}_{x,l}=\left[a+\max(0,j-3)2^{-l}h_0, a+ \min(j+2,2^ln_0)2^{-l}h_0\right] \supset \Ixl,
\] with the same $j$ as above.  Let
\begin{equation}\label{eqn:defoflx}
L(x) = \min \left\{ l \in \mathbb{N}_0 :  \frac{1}{8} \fC\left(3\cdot2^{-l}h_0\right)(2^{-l}h_0)^2\norm[\bar{I}_{x,l}]{f''} \le \abstol \right\}.
\end{equation}

Note that $L(x)$ does depend on $f$, although this dependence is suppressed in the notation.

We now show that $\ell(x) \le L(x)$.  At each iteration of Algorithm $A$, $x$ lies in $\Ixl$ for some $l$, and that by the time algorithm $A$ terminates, all values of $l = 0, \ldots, \ell(x)$ are realized.  If $\ell(x) > L(x)$, then at some iteration $I_{x,L(x)}$ must be split in Step~\ref{stage2} of $A$.  Let $\cp = \{x_i\}_{i=0}^n$ denote the partition of $[a,b]$ at this iteration, and then identify the closure of $I_{x,L(x)}$ as $[x_{i-1},x_i]$ for some $i$, where $x_i-x_{i-1}=2^{-L(x)}h_0$.
Referring to Step~\ref{stage2} of $A$, it is necessary that $i \in \mathcal{I} \cap \widetilde{\mathcal{I}}$ or  $i \in \widehat{\mathcal{I}}\cap \widetilde{\mathcal{I}}$ for $I_{x,L(x)}$ to be split.  We show that neither of these conditions can hold, thus contradicting the assumption that $\ell(x) > L(x)$.

Let us investigate these two conditions separately.
\begin{enumerate}
  \item $i \in \mathcal{I} \cap \widetilde{\mathcal{I}}$.  If $i \in \mathcal{I}_+ \cap \widetilde{\mathcal{I}}$, then
  \begin{equation}
  \label{conditionone} x_{i}-x_{i-1} \ge x_{j}-x_{j-1}, \qquad   j=i+1,i+2, \qquad [x_i,x_{i+2}] \subseteq \bar{I}_{x,L(x)}.
  \end{equation}
  This implies that
  \begin{align*}
  \abstol & <  \frac{B_{i,+}(f,\cp)}{8}(x_i-x_{i-1})^2 \\
  & \le \frac{\fC(x_{i+2}-x_{i-1})f[x_{i},x_{i+1},x_{i+2}]}{8}(x_{i}-x_{i-1})^2 \qquad \text{by}~\eqref{bpf} \\
  %\text{since } x_{i}-x_{i-1} \ge x_{j}-x_{j-1}, j=i+1,i+2 \ \ \
  &\le  \frac 1 8 \fC\left(3\cdot2^{-L(x)}h_0\right) (2^{-L(x)}h_0)^2\norm[{[x_{i},x_{i+2}]}]{f''} \qquad \text{by}~\eqref{conditionone} \\
     & \le   \frac 1 8 \fC\left(3\cdot2^{-L(x)}h_0\right) (2^{-L(x)}h_0)^2  \norm[\bar{I}_{x,L(x)}]{f''}  \qquad \text{by}~\eqref{conditionone} \\
     & \le    \abstol \qquad \text{by}~\eqref{eqn:defoflx} .
  \end{align*}
A similar argument yields $\abstol < \abstol$ for $i \in \mathcal{I}_- \cap \widetilde{\mathcal{I}}$ as well.  Thus $i \in \mathcal{I} \cap \widetilde{\mathcal{I}}$ is impossible.
  \item $i \in \widehat{\mathcal{I}}\cap \widetilde{\mathcal{I}}$\\
  There are four different cases in this situation. We first consider $i \in \widehat{\mathcal{I}}_{+1} \cap \widetilde{\mathcal{I}}$.
  Thus
  \begin{equation} \label{conditiontwo}
  x_{i}-x_{i-1} \ge x_{j}-x_{j-1},\qquad j=i-1,i+1, \qquad [x_{i-1},x_{i+1}] \subseteq \bar{I}_{x,l}.
  \end{equation}
  Then
  \begin{align*}
  \abstol & < \frac{B_{i-1,+}(f,\cp)}{8}(x_{i-1}-x_{i-2})^2 \\
& \le  \frac{\fC(x_{i+1}-x_{i-2})f[x_{i-1},x_{i},x_{i+1}]}{8}(x_{i-1}-x_{i-2})^2 \qquad  \text{by}~\eqref{bpf}   \\
 & \le  \frac 1 8 \fC\left(3\cdot2^{-L(x)}h_0\right) (2^{-L(x)}h_0)^2 \norm[{[x_{i-1},x_{i+1}]}]{f''}  \text{by}~\eqref{conditiontwo}  \\
     & \le   \frac 1 8 \fC\left(3\cdot2^{-L(x)}h_0\right) (2^{-L(x)}h_0)^2 \norm[\bar{I}_{x,L(x)}]{f''} \qquad \text{by}~\eqref{conditiontwo}  \\
     & \le  \abstol \qquad \text{by}~\eqref{eqn:defoflx}.
  \end{align*}
  We also get contradiction for other three cases. So we cannot have $i \in \widehat{\mathcal{I}}\cap \widetilde{\mathcal{I}}$.
\end{enumerate}
Hence, we prove $\ell(x) \le L(x)$. This fact is used to prove an upper bound on the computational cost of Algorithm $A$.

\begin{theorem}\label{thm:cost}
Let $\cost(A,f,\abstol)$ denote the number of functional evaluations required by Algorithm $A$ for the input function $f$ and the absolute error tolerance $\abstol$.  This algorithm is known to successfully solve problem~\eqref{appxprob} for functions in $\cc$.  The computational cost of doing so has the following upper bound:
\begin{equation*}
\cost(A,f,\abstol) \le \frac{1}{h_0}\int_a^b 2^{L(x)} \, \dif x +1 \\
\end{equation*}
where $L(x)$ is defined in~\eqref{eqn:defoflx} and
\begin{equation*}
2^{L(x)} = \min\left\{2^l:  2^l \ge \sqrt{\frac{\fC\left(3\cdot 2^{-l} h_0\right) h_0^2 \norm[\bar{I}_{x,l}]{f''} }{8 \abstol}}, \  l \in  \natzero\right\}.
\end{equation*}
\end{theorem}

\begin{proof}
Let $\cp=\{x_i\}_{i=0}^n$ be the final partition when $A(f,\abstol)$ successfully terminates. Note that $2^{\ell(x)}$ is constant on $I_{x_i,\ell(x_i)} = [x_{i},x_{i+1}[$ for $i=0, \ldots, n-1$.  Furthermore  $\int_{x_i}^{x_{i+1}} 2^{\ell(x)} \, \dif  x =  h_0$.  Then the number of function values requires is
\begin{equation*}
n+1 = 1 + \sum_{i=0}^{n-1} 1 = 1 + \sum_{i=0}^{n-1} \frac{1}{h_0} \int_{x_i}^{x_{i+1}} 2^{\ell(x)} \, \dif  x = 1 + \frac{1}{h_0}\int_a^b 2^{\ell(x)} \, \dif x.
\end{equation*}
Noting that $\ell(x) \le L(x)$ establishes the formula for $\cost(A,f,\abstol)$.  The formula for $2^{L(x)}$ follows from the definition of $L(x)$
in~\eqref{eqn:defoflx}.
\end{proof}

The definition of $L(x)$ in~\eqref{eqn:defoflx} is implicit because $l$ in the definition appears in the norm $\norm[\bar{I}_{x,l}]{f''}$.  For small $\abstol$, $L(x)$ becomes large,  $\norm[\bar{I}_{x,L(x)}]{f''}$ approaches $|f''(x)|$, and $3\cdot 2^{-L(x)}h_0$ tends to $0$.  Thus, for small $\abstol$, we have
$$ 2^{L(x)} \approx \sqrt{\frac{\fC\left(0\right)  h_0^2 |f''(x)|}{8\abstol}}.$$
Under this approximation, the upper bound on computational cost becomes
\begin{align}
\nonumber
\cost(S,f,\abstol)  &\lesssim \frac{1}{h_0}\int_a^b \sqrt{\frac{\fC\left(0\right)  h_0^2 |f''(x)|}{8\abstol}} \, \dif x +1 \\
& =\sqrt{\frac{\fC(0) \norm[\frac 12]{f''}}{8\abstol}} +1 \label{costbdapprox} \\
& \le (b-a)\sqrt{\frac{\fC(0) \norm{f''}}{8\abstol}} +1 \label{costbdinf}  \qquad \text{by \eqref{halflessinf}}.
\end{align}

For functions in the cone $\cc$, the quantities $\norm{f''}$ and $\norm[\frac 12]{f''}$ bound each other as is claimed in the following proposition.

\begin{prop} \label{equivnormprop} The quantities $\norm{f''}$ and $\norm[\frac 12]{f''}$ bound each other as follows:
	\begin{subequations}
		\begin{gather}
		\label{inflesshalf}
		\max_{0 \le h \le \fh} \frac{h^2}{\fC(h)} \norm{f''} \le \norm[\frac 12]{f''} \qquad \forall f \in \cc \\
		\label{halflessinf}
		\norm[\frac 12]{f''}  \le (b-a)^2 \norm{f''} \qquad \forall f \in \cw^{2,\infty}.
		\end{gather}
	\end{subequations}
\end{prop}
\begin{proof}
The first inequality comes from the definition of the cone:
\begin{align*}
\norm{f''} & = \sup_{a \le x \le b} \max \abs{f''(x)} \\
& \le \sup_{a \le x \le b} \min\bigl\{\tB(f,x,h,h) : h \in [0,\fh], \ a \le x - h < x+h \le b \bigr \} \quad \text{by}~\eqref{conedef} \\
& \le \min_{0 \le h \le \fh} \frac{\fC(h)}{h^2}\norm[\frac 12]{f''} \qquad \text{by}~\eqref{conedef} \text{ and}~\eqref{onebdm}.
\end{align*}
The second inequality follows directly from the definitions of the (quasi-) norms:
\begin{equation} \label{onebdm}
\norm[\frac 12]{f''} = \biggl\{ \int_{a}^{b} \sqrt{\abs{f''(x)}} \, \dif x \biggr\}^2 \le\biggl\{\sqrt{\norm{f''}}  \int_{a}^{b} \, \dif x \biggr\}^2 \le  (b - a)^2 \norm{f''}.
\end{equation}
\end{proof}

If $\fC$ is defined as in~\eqref{sampleC}, then
\[
\max_{0 \le h \le \fh} \frac{h^2}{\fC(h)} = \max_{0 \le h \le \fh} \frac{h^2(\fh - h)}{\fC(0)\fh} = \frac{4\fh^2}{29\fC(0)}.
\]
When $\fh$ is small, it is possible for $\norm[\frac 12]{f''} $ to be quite small in comparison to $\norm{f''}$.  This occurs when $f''$ is rather spiky.


Although $\norm{f''}$ and $\norm[\frac 12]{f''} $ are equivalent for $f \in \cc$, that is not the case for $f \in \cw^{2,\infty}$.  The hump function $f_3$ defined above satisfies
\[
\norm{f_3''} = \delta^{-2}, \qquad \norm[\frac 12]{f_3''}  = 16.
\]
By making $\delta$ small enough, we may make $\norm[\frac 12]{f_3''}/\norm{f_3''} = 16\delta^2$ arbitrarily small.  However, since $f_3 \notin \cc$ for $\delta < 2\fh$, this does not violate Proposition~\ref{equivnormprop}.

\subsection{Lower Complexity Bound} \label{subsec:appxcomp}

The upper bound on the computational cost of Algorithm~\ref{AlgoA} provides an upper bound on the complexity of problem~\eqref{appxprob}.  We now construct lower bounds on the complexity of the problem, i.e., the computational cost of the best algorithm.  We then observe that these lower bounds have the same aysmptotic behavior as the computational cost of algorithm $A$.

\begin{theorem}
	Let $A^*$ be any deterministic algorithm that successfully solves problem~\eqref{appxprob}.
	
	\begin{enumerate}
		\renewcommand{\labelenumi}{\roman{enumi}.}
		\item  If $A^*$ solves~\eqref{appxprob} for all $f \in \cc$, then for any $\sigma, \abstol >0$,
		\begin{subequations} \label{lowbdC}
		\begin{gather}
		\sup_{f \in \cc : \norm[\frac12]{f''} \le\sigma } \cost(A^*,f,\abstol) \ge \sqrt{\frac{(\fC(0)-1)\sigma}{16(\fC(0)+1)\abstol}} -1, \label{lowbdChalf}\\
		\sup_{f \in \cc : \norm{f''} \le\sigma } \cost(A^*,f,\abstol) \ge \sqrt{\frac{(\fC(0)-1)\sigma(b-a)^2}{16(\fC(0)+1)\abstol}} -1 \label{lowbdCinf}.
		\end{gather}
		\end{subequations}
		
		\item If $A^*$ solves~\eqref{appxprob} for all $f \in  \cw^{2,\infty}$, then for any $\sigma, \abstol >0$,
		\begin{subequations} \label{lowbdW}
		\begin{gather}
		\sup_{f \in \cw^{2,\infty} : \norm[\frac12]{f''} \le \sigma} \cost(A^*,f,\abstol) = \infty \quad \text{for } \sigma > 16 \abstol, \label{lowbdWhalf}\\
		\sup_{f \in \cw^{2,\infty} : \norm{f''} \le\sigma } \cost(A^*,f,\abstol) \ge \frac{(b-a)}{4} \sqrt{\frac{\sigma}{\abstol}} - 1.  \label{lowbdWinf}
		\end{gather}
		\end{subequations}
				
	\end{enumerate}
	\label{thm:A_cost}
\end{theorem}

Note that restricting the set of interesting functions from all of $ \cw^{2,\infty}$ to just the cone $\cc$ can significantly affect the lower complexity bounds.  Also note that the conclusions of this theorem hold whether or not $\sigma$ is an input to the algorithm $A^*$.  For our algorithm $A$, $\sigma$ is \emph{not} an input. Moreover, the upper bound on the computational cost of $A$ is asymptotically  However, one can easily construct an algorithm

\begin{proof}
	All of these lower bounds are proved by constructing fooling functions for which algorithm $A$ succeeds, and then showing that at least a certain number of samples must be used.  The proofs of lower bounds~\eqref{lowbdW} are simpler, so we start with them first.
	
	Let $A^*$ be a successful algorithm for all $f \in  \cw^{2,\infty}$, and consider the partition $\{x_i\}_{i=0}^{n+1}$, $a=x_0 \le x_1 < \cdots < x_n \le x_{n+1} = b$, where $\{x_i\}_{i=1}^n$ are the data sites used to compute  $A^*(0,\abstol)$.  Choose any $j=1, \ldots, n+1$ with $x_j-x_{j-1} \le (b-a)/(n+1)$.  Let $f_3$ be defined as in~\eqref{f3def} with $c = (x_j+x_{j-1})/2$, and $\delta  = (b-a)/[4(n+1)]$.  Note that in general
	\begin{equation}
	\norm{f_3''} = \frac{1}{\delta^{2}}, \qquad \norm[\frac 12]{f_3''} = 16.
	\end{equation}

For any real $\gamma$, it follows that $\gamma f_3(x_i)=0$ for $i=0, \ldots, n+1$.  Figure \ref{f3foolplot} illustrates this situation.  Since $0$ and $\pm \gamma f_3$ share the same values at the data sites, then they must share the same approximation: $A^*(\pm \gamma f_3,\abstol) = A^*(0,\abstol)$.  Moreover, $\cost(A^*,0,\abstol) = \cost(A^*,\pm \gamma f_3,\abstol) = n$.  This implies  that $\gamma$ must be no greater than $\abstol$:
	\begin{align*}
	\abstol  &\ge \max(\norm{\gamma f_3 - A^*(\gamma f_3,\abstol)}, \norm{-\gamma f_3 - A^*(-\gamma f_3,\abstol)}) \\
	&= \max(\norm{\gamma f_3 - A^*(0,\abstol)}, \norm{-\gamma f_3 - A^*(0,\abstol)}) \\
	& \ge \frac{1}{2} [\norm{\gamma f_3 - A^*(0,\abstol)} + \norm{-\gamma f_3 - A^*(0,\abstol)}] \\
	& \ge \frac{1}{2} \norm{\gamma f_3 - (-\gamma f_3)} =  \norm{\gamma f_3} = \gamma \\
	& = \begin{Bmatrix} \displaystyle \frac{ \norm[\frac 12]{\gamma f_3''}}{16}  \\
	\displaystyle \delta^2 	\norm{\gamma f_3''} =  \frac{(b-a)^2 \norm{\gamma f_3''}}{16(n+1)^2}
	\end{Bmatrix}.
	\end{align*}
The top inequality cannot be satisfied unless $\sigma = \norm[\frac 12]{\gamma f_3''}$ is small enough.  Solving the bottom inequality for $n$ in terms of $\sigma =  \norm{\gamma f_3''}$ establishes~\eqref{lowbdWinf}.
	
\begin{figure}
	\centering
	\includegraphics[width = 8cm]{figure/f3foolplot.eps}
	\caption{The fooling functions $\pm f_3$ used to prove \eqref{lowbdW}.  The case $n=15$ is shown.  \label{f3foolplot}}
\end{figure}
	
Now we prove the lower complexity bounds~\eqref{lowbdC}, assuming that $A^*$ be a successful algorithm for all $f  \in \cc$.  Let $f_0$ be defined as follows
\begin{equation}
\label{assumpfzero}
f_0(x) =\frac{x^2}{2}, \quad f''(x) = 1, \quad x \in [a,b], \qquad \norm[\frac12]{f_0''} = (b -a)^2  \quad \norm{f_0''} = 1.
\end{equation}
Since $f''$ is constant, $f \in \cc$, and so $A^*$ successfully approximates $\gamma f_0$ for any non-negative $\gamma$.

Consider the partition $\{x_i\}_{i=0}^{n+1}$, $a=x_0 \le x_1 < \cdots < x_n \le x_{n+1} = b$, where $\{x_i\}_{i=1}^n$ are the data sites used to compute  $A^*(\gamma f_0,\abstol)$.    Again choose any $j=1, \ldots, n+1$ with $x_j-x_{j-1} \le (b-a)/(n+1)$, and let $f_3$ be defined as in~\eqref{f3def} with $c = (x_j+x_{j-1})/2$, and $\delta  = (b-a)/[4(n+1)]$. We construct two fooling functions:
\begin{gather*}
f_{\pm} = f_0 \pm \tgamma f_3, \quad \tgamma =\frac{\fC(0)-1}{\fC(0)+1}\delta^2, \\
\norm{f''_{\pm}} = 1+\frac{\tgamma}{\delta^2} = \frac{2\fC(0)}{\fC(0)+1} , \\
m(f''_{\pm},\alpha,\beta) \ge 1-\frac{\tgamma}{\delta^2} = \frac{2}{\fC(0)+1} = \frac{\norm{f''_{\pm}} }{\fC(0)}.
\end{gather*}
The above calculations show that $\gamma f_{\pm} \in \cc$ as well.  Moreover, the definition of $f_{\pm}$ ensures that
$A^*(\gamma f_0) = A^*(\gamma,f_{\pm})$, and $\cost(A^*,\gamma f_0) = \cost(A^*,\gamma f_{\pm}) = n$.

Again we make the argument that $\gamma$ must be smaller than :
\begin{align*}
\abstol & \ge \max(\norm{\gamma f_+-A(\gamma f_+,\abstol)},\norm{\gamma f_--A(f_-, \abstol)}) \\
& \ge \frac{1}{2} \left[ \norm{\gamma f_+-A(\gamma f_+,\abstol)}+ \norm{\gamma f_--A(f_-, \abstol)} \right] \\
& = \frac{1}{2} \left[ \norm{\gamma f_+-A(\gamma f_0,\abstol)}+ \norm{\gamma f_--A(f_0, \abstol)} \right] \\
& \ge \frac{1}{2}  \norm{\gamma f_+ - \gamma f_-} =  \norm{\gamma \tgamma f_3} = \gamma \tgamma\\
& = \begin{Bmatrix} \displaystyle
\frac{\norm[\frac 12]{f_0''}}{(b-a)^2}, \\
\norm{f_0''}
\end{Bmatrix}  \cdot \frac{(\fC(0)-1)\delta^2}{\fC(0)+1} \\
& = \begin{Bmatrix} \displaystyle
\norm[\frac 12]{f_0''}, \\
(b-a)^2\norm{f_0''}
\end{Bmatrix}  \cdot \frac{\fC(0)-1}{16(\fC(0)+1)(n+1)^2}
\end{align*}
Substituting $\norm[\frac12]{f_0''} = \sigma$ in the top inequality and $\norm{f_0''} = \sigma$ in the bottom inequality, and then solving for $n$ yields the two bounds in~\eqref{lowbdC}.
\end{proof}

Comparing the approximate upper bounds on the computational cost of Algorithm $A$ in  \eqref{costbdapprox} and \eqref{costbdinf} to these lower bounds for the best possible algorithm, $A^*$, we find that they are asymptotically equivalent:
\begin{multline*}
\frac{\displaystyle \sup_{f \in \cc : \norm[\frac 12]{f} \le \sigma} \cost(A,f,\abstol)}{\displaystyle	\sup_{f \in \cc : \norm[\frac12]{f''} \le\sigma } \cost(A^*,f,\abstol) } \approx
\frac{\displaystyle \sup_{f \in \cc : \norm{f} \le \sigma} \cost(A,f,\abstol)}{\displaystyle	\sup_{f \in \cc : \norm{f''} \le\sigma } \cost(A^*,f,\abstol) } \\ \lessapprox \sqrt{\frac{2 \fC(0)(\fC(0)+1)}{\fC(0)-1}}.
\end{multline*}
The right hand side of this inequality does depend on how far $\fC(0)$ is above $1$, but it does not depend on how small $\fh$ is.

\section{The Minimization Algorithm, $M$} \label{sec:funmin}

\subsection{Algorithm $M$}  \label{sec:minalgo}
\xinnote{This is the same assumption and description as section 3.1}\\
Instead of defining the cone in terms of $\fh$ directly, we choose an initial number of points
\begin{equation*}
	n_{\ninit} = \max\left\{\left\lceil n_{\text{hi}}
	\left(\frac{n_{\lo}}{n_{\text{hi}}}\right)^{\frac{1}{1+b-a}}\right\rceil ,5\right\}.
\end{equation*}
that lies somewhere between $n_{\lo}$ and $n_{\text{hi}}$, depending on the width $[a,b]$.  Then, we choose $\fh := 5(b-a)/(n_{\ninit}-1)$.  In this way, $n_{\ninit} -1$ becomes the minimum number of sub-intervals used by the algorithm.  A higher $n_{\ninit}$ corresponds to a more robust algorithm.  By the arguments of the previous section, the following algorithm solves function minimization problem~\eqref{optprob}.

\begin{algoM} \label{AlgoM}
	For some finite interval, $[a,b]$, some fixed integers $n_{\lo}, n_{\text{hi}}$ satisfying $5 \le n_{\lo} \le n_{\text{hi}}$, and some non-decreasing
	$\fC:[0,b-a] \to [1, \infty)$, let $f:[a,b] \to \reals$ and $\abstol >0$ be
	user inputs. Choose $\fh := 5(b-a)/(n_{\ninit}-1)$, where $n_{\ninit}$ is defined in~\eqref{nodefinition}, and let  $n=n_{\ninit}-1$.
	Define the partition, $\cp$ of  equally spaced points and the index set of intervals for which the error tolerance
	is not yet satisfied:
	$$x_i=a+\frac{i}{n}(b-a), \ i=0,\ldots,n, \qquad
	\mathcal{I} = \{1,2,\ldots,n-1,n\}.$$
	Choose the inflation function $\fC$. Compute the second-order divided difference, $f[x_{i-1},x_{i}, x_{i+1}]$, for $i = \{1,2,\ldots,n-1\}$. Then do the following:
	\begin{enumerate}[\bf Step 1.]%\hspace{8.5ex}
		\item \label{stagemin1} Check for convergence.
		Compute $B_{i,\pm}(f,\cp)$ as in~\eqref{bpf} and~\eqref{bmf}, where $i \in \mathcal{I}$. Compute $\text{fmin} = \minfi$.
		Let
		\[
		\mathcal{I}_\pm = \left\{i \in \mathcal{I}: \minfii -\frac{(x_i - x_{i-1})^2B_{i,\pm}(f,\cp)}{8}  < \text{fmin}-\abstol \right\}.
		\]
		Then update $\ci$ to be $\mathcal{I}_+ \cup \mathcal{I}_-$.  If $\mathcal{I} = \emptyset$, return $M(f,\abstol) = \text{fmin}$ and terminate the algorithm.
		Otherwise, continue to the next step.
		\item \label{stagemin2}
		Denote $\widehat{\mathcal{I}}=\widehat{\mathcal{I}}_{+1} \cup \widehat{\mathcal{I}}_{+2} \cup \widehat{\mathcal{I}}_{-1} \cup \widehat{\mathcal{I}}_{-2},$ where
		\begin{multline*}
		\widehat{\mathcal{I}}_{\pm k} = \left\{i \in \{1,2,\ldots,n\}:  \minfii -\frac{(x_j - x_{j-1})^2B_{j,\pm}(f,\cp)}{8}  < \text{fmin}-\abstol,\right.\\
		\left.j=i\mp k, 0<j\le n, k=1,2. \right\},
		\end{multline*}
		\[\widetilde{\mathcal{I}}=\left\{i \in \mathcal{I} \cup \widehat{\mathcal{I}}: x_i - x_{i-1}=\max\limits_{j \in \mathcal{I} \cup \widehat{\mathcal{I}} } x_j-x_{j-1} \right\}.\]
		Split in half those intervals $[x_{i-1},x_i]$ for which $i$ lies in $\widetilde{\mathcal{I}}$.
		Update $n$, $\cp = \{x_i\}_{i=0}^n$, $\mathcal{I}$, and the $f[x_{i-1}, x_{i}, x_{i+1}]$ accordingly.  Return to Step~\ref{stagemin1}.
	\end{enumerate}
\end{algoM}

\begin{theorem} \label{thm:algMworks}
Algorithm $M$ defined above solves problem~\eqref{optprob} for functions in the cone $\cc$ defined in~\eqref{conedef}.
\end{theorem}
\xinnote{
we need a theorem that works.\\
Numerical examples.\\
We need computational cost\\
complexity bound
}

\subsection{Computational Cost} \label{subsec:optcost}

\subsection{Lower Complexity Bound} \label{subsec:optcomp}



\section{Numerical Examples}

\input{examples}

\section{Discussion}

\input{discussion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{References}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliography{FJH23,FJHOwn23}

\end{document}








