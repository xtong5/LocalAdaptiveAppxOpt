\documentclass[review]{elsarticle}

\usepackage{amsmath,amssymb,amsthm,xspace,mathtools,hyperref,color}

\input{FJHDef.tex}

\journal{Journal of Complexity}

%%%%%%%%%%%%%%%%%%%%%%%
%% Elsevier bibliography styles
%%%%%%%%%%%%%%%%%%%%%%%
%% To change the style, put a % in front of the second line of the current style and
%% remove the % from the second line of the style you would like to use.
%%%%%%%%%%%%%%%%%%%%%%%

%% Numbered
%\bibliographystyle{model1-num-names}

%% Numbered without titles
%\bibliographystyle{model1a-num-names}

%% Harvard
%\bibliographystyle{model2-names.bst}\biboptions{authoryear}

%% Vancouver numbered
%\usepackage{numcompress}\bibliographystyle{model3-num-names}

%% Vancouver name/year
\usepackage{numcompress}\bibliographystyle{model4-names}\biboptions{authoryear}

%% APA style
%\bibliographystyle{model5-names}\biboptions{authoryear}

%% AMA style
%\usepackage{numcompress}\bibliographystyle{model6-num-names}

%% `Elsevier LaTeX' style
%\bibliographystyle{elsarticle-num}
%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\abstol}{\varepsilon_{\textrm{a}}}
\newcommand{\datasites}{\{x_i\}_{i=0}^n}
\makeatletter
\newcommand{\vast}{\bBigg@{4}}
\newcommand{\Vast}{\bBigg@{6}}
\makeatother
%\newdefinition{algo}{Algorithm}
\theoremstyle{definition}
\newtheorem*{algoA}{Algorithm $A$}
\newcommand{\vastl}{\mathopen\vast}
\newcommand{\vastm}{\mathrel\vast}
\newcommand{\vastr}{\mathclose\vast}
\newcommand{\Vastl}{\mathopen\Vast}
\newcommand{\Vastm}{\mathrel\Vast}
\newcommand{\Vastr}{\mathclose\Vast}

\definecolor{orange}{rgb}{1.0,0.3,0.0}
\definecolor{violet}{rgb}{0.75,0,1}
\definecolor{darkgreen}{rgb}{0,0.6,0}
\newcommand{\frednote}[1]{  {\textcolor{red}  {\mbox{**Fred:} #1}}}
\newcommand{\yuhannote}[1]{  {\textcolor{darkgreen}  {\mbox{**Yuhan:} #1}}}
\newcommand{\xinnote}[1]{ {\textcolor{violet}  {\mbox{**Xin:} #1}}}
\newcommand{\scnote}[1]{ {\textcolor{orange}  {\mbox{**SC:} #1}}}

\newcommand{\Ixl}{I_{x,l}}
\newcommand{\Ixlx}{I_{x,\ell(x)}}
\newcommand{\Ixrlx}{I_{x,\rell(x)}}
\newcommand{\Ixhlx}{I_{x,\hell(x)}}
\newcommand{\hell}{\hat{\ell}}
\newcommand{\tell}{\tilde{\ell}}
\newcommand{\rell}{\mathring{\ell}}
\DeclareMathOperator{\lo}{lo}
\DeclareMathOperator{\errest}{errest}
\newtheorem{theorem}{Theorem}

\begin{document}

\begin{frontmatter}

\title{Local Adpation for Approximation and Optimization of Univariate Functions}


%% Group authors per affiliation:
\author{Sou-Cheng Choi}
\author{Yuhan Ding}
\author{Fred J. Hickernell}
\author{Xin Tong}
\address{Department of Applied Mathematics, Illinois Institute of Technology, RE 208, 10 West 32$^{\text{nd}}$ Street, Chicago, Illinois, 60616, USA}

\begin{abstract}
Some ideas to get us going
\end{abstract}

\begin{keyword}
\sep \sep
\MSC[2010]  \sep
\end{keyword}

\end{frontmatter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction} \label{sec:intro}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We want to solve two problems. For some suitable subset, $\cc$, of continuous,
real-valued functions on the finite interval $[a,b]$, we want to construct
algorithms $A:(\cc,(0,\infty)) \to L^{\infty}[a,b]$ and $M: (\cc,(0,\infty)) \to
\reals$ such that for all $f \in \cc$ and $\abstol > 0$
\begin{gather}
\norm[\infty]{f - A(f,\abstol)} \le \abstol,  \tag{APP} \label{appxprob} \\
M(f,\abstol) - \min_{a \le x \le b} f(x)  \le \abstol. \tag{MIN} \label{optprob}
\end{gather}
Here $A$ and $M$ depend only on sampled function values. They are adaptive and iterative
algorithms, which choose the data sites in $[a,b]$ adaptively, depending on the
function values already obtained at the data sites chosen in the previous iteration already. These
algorithms also automatically determine when to stop sampling the input function
and return the answers.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Cone, $\cc$} \label{sec:cone}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Linear splines will form the basis for our adaptive algorithms $A$ and $M$, which solve the
function approximation and minimization problems to the given tolerance, as
specified in \eqref{appxprob} and \eqref{optprob}. The best linear spline error possible occurs for the Sobolev space of functions whose second derivatives have finite $L^{\infty}$-norm:
\[
\cw^{2,\infty}[a,b] : = \Bigl \{f \in C^1[a,b] : \norm{f''} : = \norm[{[a,b]}]{f''} : = \sup_{a \le x \le b} \abs{f''(x)} <  \infty \Bigr\}.
\]

To bound the error of the linear spline, our adaptive algorithms construct data-based upper bounds on $\norm[{[\alpha,
\beta]}]{f''}$ in terms of function values.  In \eqref{normbd} below, we derive an upper bound $B$, such that
\[
\norm[{[\alpha, \beta]}]{f''} \le B(f,[\alpha,\beta]) \quad \forall f \in \cc, \ a \le \alpha < \beta \le b,
\]
where $B$ depends only on values of $f$ sampled in the neighborhood of $[\alpha,
\beta]$. The set $\cc \subset \cw^{2,\infty}[a,b]$ consists of functions that are \emph{not too spiky}.  Our upper bound is homogeneous in the function, i.e.,
$B(cf,[\alpha,\beta]) = \abs{c} B(f,[\alpha,\beta])$ for all real $c$. Thus, the
set of functions $\cc$ for which our bound applies---and our algorithms
succeed---is chosen to be a cone: $f \in \cc \implies cf \in \cc$ for all real
$c$. The precise definition of $\cc$ is given below in \eqref{conedef}. Cones of
functions are key to the theoretically justified adpative algorithms in
\cite{HicEtal14b}, \cite{Ton14a}, \cite{Din15a}, and \cite{HicRazYun15a}.

%--------------------------------------------------
\subsection{Newton Divided Differences} \label{sec:ndd}
%--------------------------------------------------

For any $\{x_1, x_2, x_3\}$ satisfying $a \le x_1 < x_2 < x_3 \le b$, we define a
measure of how small $\abs{f''}$ can be in the interval $[x_1, x_3]$ as follows:
\begin{equation} \label{minfppdef}
m(f,[x_1, x_3]) = \min \Bigl\{ \abs{z}  : \inf_{x_1 \le x \le x_3} f''(x) \le z \le \sup_{x_1 \le x \le x_3} f''(x) \Bigr\}.
\end{equation}
Here the infimum and supremum exclude points where $f''$ is undefined. Note that
$m(f,[x_1, x_3])$ is either $ \inf_{x_1 \le x \le x_3} \abs{f''(x)}$ or $0$
depending on whether $f''$ takes on one sign or both signs on the interval
$[x_1, x_3]$. Because of possible jump discontinuities in $f''$, $m(f,[x_1,
x_3])$ may be $0$ even when $\inf_{x_1 \le x \le x_3} \abs{f''(x)} > 0$. Note
that for any intervals $I , J \subseteq [a,b]$, it follows from the definition~\eqref{minfppdef}
that
\begin{equation} \label{mdec}
m(f,I) \ge m(f,J) \qquad \forall I \subseteq J.
\end{equation}

We cannot determine $f''(x)$ for a specific $x$ based only on values of $f$. However,
we can use values of $f$ to determine $f''(x)$ \emph{for some unknown $x$} in a
specified interval via the Newton divided differences.


Let $p$ denote the Lagrange quadratic interpolating polynomial at the nodes
$\{x_1, x_2, x_3\}$, which may be written as
\begin{equation*}
p(x) : = f[x_1] + f[x_1, x_2](x-x_1) + f[x_1, x_2, x_3](x-x_1)(x-x_2),
\end{equation*}
where the $f[\cdots]$ are the Newton divided differences; see for example~\cite{CheKin12a}. In particular,
\begin{gather}
\nonumber
f[x_1] = f(x_1), \qquad f[x_1, x_2] = \frac{f[x_2] - f[x_1]}{x_2-x_1},  \\
f[x_1, x_2,x_3] = \frac{f[x_2,x_3] - f[x_1,x_2]}{x_3-x_1}. \label{divdiff}
\end{gather}
For any $f \in
\cw^{2,\infty}[a,b]$, the function $f - p$ has at least three distinct zeros on
$[x_1, x_3]$, so $f' - p'$ has at least two distinct zeros on this interval,
i.e., there exist $\xi_\pm$ with $x_1 < \xi_- < x_2 < \xi_+ < x_3$ with
$f'(\xi_\pm) - p'(\xi_{\pm}) = 0$. If $f''$ is continuous, then we can conclude
that $ f[x_1, x_2, x_3]= p''(\zeta) =f''(\zeta) $ for some $\zeta \in [x_1,
x_3]$. However, $\cw^{2,\infty}[a,b]$ contains functions without continuous
second derivatives. Fortunately, we can obtain a somewhat weaker---yet equally useful result---by the definition of \eqref{minfppdef},
\begin{multline*}
\bigabs{f[x_1, x_2, x_3]}(\xi_+  - \xi_-) = \abs{p'(\xi_+) - p'(\xi_-)} =  \abs{f'(\xi_+) - f'(\xi_-)} \\
= \abs{\int_{\xi_-}^{\xi_+} f''(x) \, \dif x} \ge m(f,[x_1, x_3]) (\xi_+  - \xi_-).
\end{multline*}
So the the data-based second-order divided difference provides an upper
bound on how small $\abs{f''}$ can be in the interval $[x_1, x_3]$:
\begin{equation}
\bigabs{f[x_1, x_2, x_3]} \ge m(f,[x_1, x_3]).
\end{equation}

%--------------------------------------------------
\subsection{The Cone Definition}  \label{sec:conedef}
%--------------------------------------------------

Let $\fh$ be some positive number, and let $\fC : [0,b-a] \to [1,\infty)$ be any
non-decreasing function. The cone of functions for which our algorithms will
apply only includes functions whose second derivatives do not change much in
magnitude over a short distance, namely,\yuhannote{change the definition of cone}
\begin{align*} \label{conedef}
\nonumber \cc :=   \Bigl \{
 f  \in \cw^{2,\infty}[a,b]: &  \  \text{for all } x \in [a,b], \ x_\pm = x \pm h_\pm,
  \  \max(h_{\pm}) \in [0, \fh], 
\\  & \limsup_{t \to x}\abs{f''(t)}  \le M(f,x,h_\pm) \Bigr \},
\end{align*}
where
\begin{multline*}
M(f,x,h_\pm)=\\
\begin{cases}
  \max\bigl(\fC(h_{-}) m(f,[x_-,x]),\fC(h_{+}) m(f,[x,x_+])\bigr), & a \le x_- \le x_+ \le b\\
\fC(h_{-}) m(f,[x_-,x]), & a \le x_- \le b <  x_+\\
\fC(h_{+}) m(f,[x,x_+]), & x_- < a \le x_+ \le b
\end{cases}. %\quad
 %\Vastr
\end{multline*}
The parameter $\fh$ and the function $\fC$ define the cone $\cc$; increasing
one or both expands the cone to include more functions.

A function $f$ with $f''(\alpha) = f''(\beta) = 0 \ne f''((\alpha+\beta)/2)$ may
lie inside $\cc$ only if $\beta - \alpha > 2\fh$. Except near the endpoints of
the interval, the definition of $\cc$ uses values of $f''$ on both sides of $x$
to bound $\abs{f''(x)}$. This allows $\cc$ to include functions with step
discontinuities in their second derivatives, provided that these discontinuities
do not occur too close to each other or too close to the ends of the interval.

To better understand $\cc$, we provide examples of functions lying outside $\cc$
and similar functions lying inside $\cc$. Consider these two functions defined
on $[-1,1]$ whose second derivatives oscillate wildly near $0$:
\begin{gather*}
f_1(x) = x^4 \sin(1/x), \qquad f_1''(x) = (12x^2 - 1) \sin(1/x) -6 x \cos(1/x), \\
f_2(x) = 10  x^2 + f_1(x), \qquad f_2''(x) = 20+ f_1''(x).
\end{gather*}
These functions are plotted in Figure \ref{f1f2fig}. Because the $f''_1(x)$
takes on both signs for $x$ arbitrarily close to $0$ and on either side of $0$,
it follows that $m(f_1,[-h_-,0]) = m(f_1,[0,h_+]) = 0$ for all $h_\pm \in [0,1]$.
However, $\limsup_{t \to 0} \abs{f''_1(t)} = 1$. Thus, $f_1$ cannot lie inside
$\cc$ no matter how $\fh$ and $\fC$ are defined. On the other hand,
$m(f_2,[\alpha, \beta]) \ge 13.5$ for all $-1 \le \alpha < \beta \le 1$, and
$\limsup_{t \to x} \abs{f''_2(t)} \le 27$ for all $x \in [-1,1]$, so $f_2 \in
\cc$ if $\fC(0) \ge 2$.

\begin{figure}[t]
\centering
\includegraphics[width=5.6cm]{f1f2plot.eps} \quad
\includegraphics[width=5.9cm]{f1closeplot.eps} \\
\includegraphics[width=5.6cm]{f1ppf2ppplot.eps} \quad
\includegraphics[width=5.9cm]{f2closeplot.eps}
\caption{The examples $f_1$ and $f_2$ and their second derivatives. Note that
$f_2''(x) = f_1''(x) + 20$.}
\label{f1f2fig}
\end{figure}

Consider the following function defined on $[-1,1]$, whose second derivative has jump discontinuities:
\begin{align*}
f_3(x) & = \begin{cases} \displaystyle
   \frac{1}{2\delta^2} \Bigl [4 \delta^2 + (x-c)^2 + (x-c-\delta)\abs{x-c-\delta}
\\ \qquad \qquad
    - (x-c+\delta)\abs{x-c+\delta} \Bigr ], & \abs{x-c} \le 2\delta,
\\ 0, & \text{otherwise},
\end{cases} \\
f''_3(x) & =
\begin{cases} \displaystyle
    \frac{1}{\delta^2} [1 + \sign(x-c-\delta) - \sign(x-c+\delta), & \abs{x-c} \le 2\delta,
\\ 0, & \text{otherwise}.
\end{cases}
\end{align*}
Here $c$ and $\delta$ are parameters. This function and its second derivative
are shown in Figure \ref{f3fig} for $-c=\delta = 0.2$. For this choice of
parameters, if $\fh < 0.1$, $h_\pm \in [0,\fh]$, and $x_\pm = x\pm h_\pm$, then
for $x \in [-0.6, 0.2]$ it follows that $-1 \le x_- \le x_+ \le 1$, and
\[
\max\bigl(m(f_3,[x_-,x]),m(f_3,[x,x_+])\bigr) = \sup_{-1 \le x \le 1} \abs{f_3''(x)}  = 25, \qquad -0.6 \le x \le 0.2.
\]
For $x \in [-1,1] \setminus [-0.6, 0.2]$ we note that $f_3''(x) = 0$. Thus, for
any choice of $\fC : [0,b-a] \to [1,\infty)$, the function $f_3$ lies in $\cc$.
However, if $\fh$ is too large, say, $0.1 < h_\pm \le \fh$, and $x_\pm = -0.5\pm
h_\pm$, then $m(f_3,[x_-,-0.5])=m(f_3,[-0.5,x_+])=0$. Since $f_3''(-0.5) = 25$,
now $f_3 \notin \cc$. This example illustrates how $\fh$ determines the width of
a spiky function that may still lie in $\cc$.

\begin{figure}[t]
\centering
\includegraphics[width=5.7cm]{f3plot.eps} \quad
\includegraphics[width=5.7cm]{f3ppplot.eps}
\caption{The example $f_3$ with $-c=\delta = 0.2$  and its piecwise constant second derivative.}
\label{f3fig}
\end{figure}

The above examples of functions outside $\cc$ have discontinuities in the second
derivative. Even with a function has sufficient smoothness, it \scnote{incomplete}

%--------------------------------------------------
\subsection{The Linear Spline and Its Error}
%--------------------------------------------------

Define a \emph{partition} of an interval $[a, b]$, denoted $\datasites$, to be
an ordered sequence of points that includes the endpoints of the interval,
$a=:x_0 < x_1 < \cdots < x_{n-1} < x_{n}:=b$, where $x_{i+1}-x_{i-2} <\fh, \forall i=2,\ldots, n-1$.  The linear spline
approximation to a function $f$ based on the partition $\datasites$ is denoted,
$S(f,\datasites)$ and defined as
\begin{multline} \label{splinedef}
S(f,\datasites)(x) =  \frac{x-x_i}{x_{i-1} - x_i} f(x_{i-1}) + \frac{x-x_{i-1}}{x_{i} - x_{i-1}}f(x_i), \\ x_{i-1} \le x \le x_i, \ i=1, \ldots, n.
\end{multline}
The error of this approximation is bounded as follows
\begin{equation} \label{appxerrbda}
\norm{f - S(f,\datasites)} \le \max_{i=1, \ldots, n} \frac{(x_i - x_{i-1})^2\norm[{[x_{i-1},x_i]}]{f''}}{8}.
\end{equation}
To construct an adaptive algorithm, one requires an upper bound on
$\norm[{[x_{i-1},x_i]}]{f''}$ in terms of function values. This can be done for
$f \in \cc$ using the correspondence between a second order difference and the
second derivative in~\eqref{minfppdef}.

For all $ x \in [x_{i-1},x_i]$,  denote
\begin{align*}
&h_- = x - x_{i-3}, \qquad x_- = x_{i-3},  \qquad i=3,4,\ldots,n,\\
 &h_+ = x_{i+2} - x, \qquad x_+ =  x_{i+2}, \qquad i=1,2,\ldots,n-2.
\end{align*}
Let
\begin{align*}
B_{+}(f,[x_{i-1},x_i])&=\begin{cases}
    \fC(x_{i+2}-x_{i-1})\abs{f[x_i,x_{i+1},x_{i+2}]},  & i=1,\ldots,n-2,
\\ 0, & i=n-1,n.
\end{cases}\\
 B_{-}(f,[x_{i-1},x_i])&=\begin{cases}
   0,  & i=1,2,
\\ \fC(x_{i}-x_{i-3})\abs{f[x_{i-3},x_{i-2},x_{i-1}])}, & i=3,\ldots,n.
\end{cases}
\end{align*}
At first, we consider the case of an interior interval, i.e., $i \in \{3, \ldots, n-2\}$. Then it follows by the definition of the cone that
\begin{equation*}
\abs{f''(x)} \le \max\bigl(\fC(h_-)m(f,[x_{i-3},x]),\fC(h_+)m(f,[x,x_{i+2}])\bigr)  \quad  \forall x \in [x_{i-1},x_i].
\end{equation*}
Applying the fact that $\fC$ is non-decreasing and property \eqref{mdec} yields
the following upper bound for the norm of $f''$ in terms of function values:
\begin{align}\label{normbd}
\nonumber
\MoveEqLeft{\norm[{[x_{i-1},x_i]}]{f''}}\\
\nonumber
 \le  & \sup_{x_{i-1} \le x \le x_i} \bigl[\max\bigl(\fC(x-x_{i-3})m(f,[x_{i-3},x]),\fC(x_{i+2}-x)m(f,[x,x_{i+2}])\bigr)\bigr]  \\
 %\nonumber & \qquad \qquad \times \max\bigl(m(f,[x_{i-3},x]),m(f,[x,x_{i+2}])\bigr] \\
\nonumber
 \le  &  \max\bigl(\fC(x_{i}-x_{i-3})m(f,[x_{i-3},x_{i-1}]),\fC(x_{i+2}-x_{i-1})m(f,[x_i,x_{i+2}])\bigr) \\
\nonumber  \le & \max\bigl(\fC(x_{i}-x_{i-3})\abs{f[x_{i-3},x_{i-2},x_{i-1}])},\bigr.  
 \fC(x_{i+2}-x_{i-1})\abs{f[x_i,x_{i+1},x_{i+2}]}\bigr) \\
 =  & \max\bigl(B_{\pm}(f,[x_{i-1},x_i])\bigr).
\end{align}

Similarly, we can obtain the same result for the sub-intervals on the left and right borders of the interval. Hence,
\eqref{normbd} is satisfied for all $i=1,\ldots,n$.
The bound in \eqref{normbd} combined with \eqref{appxerrbda} yield the
data-driven error bound for the linear spline:
\begin{equation} \label{appxerrbdb}
\norm{f - S(f,\datasites)} \le
\max_{i=1, \ldots, n} \frac{(x_i - x_{i-1})^2\max\bigl(B_{\pm}(f,[x_{i-1},x_i])\bigr)}{8} .
\end{equation}
The goal is to increase the number of nodes in the partition as needed to make
this error bound smaller than the desired tolerance.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Function Approximation Algorithm, $A$}\label{sec:fappx}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{algoA}
For some finite interval, $[a,b]$, some fixed $\fh >0$, and some non-decreasing
$\fC:[0,b-a] \to [1, \infty)$, let $f:[a,b] \to \reals$ and $\abstol >0$ be
user inputs. Choose positive integers $n_{\lo}$ and $n_{\text{hi}}$ such that 
$n_{\text{hi}} \ge n_{\lo}$ and let $n=n_0-1$ where
\begin{equation}
\label{nodefinition}
n_0 = \max\left\{\left\lceil n_{\text{hi}}
\left(\frac{n_{\lo}}{n_{\text{hi}}}\right)^{\frac{1}{1+b-a}}\right\rceil ,5\right\}.
\end{equation}
Define the
following $n+1$ points and index set of intervals for which the error tolerance
is not yet satisified: $$x_i=a+\frac{i}{n}(b-a), \ i=0,\ldots,n, \qquad
\mathcal{I} = \{1,2,\ldots,n-1,n\},$$ and choose $\fC$. Compute $f[x_{i-1},
x_{i}, x_{i+1}]$ for $i = \{1,2,\ldots,n-1\}$ as in \eqref{divdiff}. Then do the
following:
\begin{enumerate}[\bf Step 1.]%\hspace{8.5ex}
%\renewcommand{\labelenumi}{\textbf{Step \arabic{enumi}.}}
\item \label{stage1} Check for convergence.
Let $\mathcal{I}=\mathcal{I}_+ \cup \mathcal{I}_-$, where
\[
\mathcal{I}_\pm = \left\{i \in \mathcal{I}: \frac{(x_i - x_{i-1})^2B_\pm(f,[x_{i-1},x_i])}{8}  > \abstol \right\}.
\]
If $\mathcal{I} = \emptyset$, return the linear spline $A(f,\varepsilon) = S(f, \{x_i\}_{i=0}^n)$ and terminate the algorithm.
Otherwise, continue to the next step.
\item \label{stage2}
Denote $\widehat{\mathcal{I}}=\widehat{\mathcal{I}}_{+1} \cup \widehat{\mathcal{I}}_{+2} \cup \widehat{\mathcal{I}}_{-1} \cup \widehat{\mathcal{I}}_{-2},$ where
\begin{multline*}
\widehat{\mathcal{I}}_{\pm k} = \left\{i \in \{1,2,\ldots,n\}: \frac{(x_j - x_{j-1})^2B_\pm(f,[x_{j-1},x_j])}{8}  > \abstol,\right.\\
 \left.j=i\mp k, 0<j\le n, k=1,2. \right\},
\end{multline*}
\[\widetilde{\mathcal{I}}=\left\{i \in \mathcal{I} \cup \widehat{\mathcal{I}}: x_i - x_{i-1}=\max\limits_{j \in \mathcal{I} \cup \widehat{\mathcal{I}} } x_j-x_{j-1} \right\}.\]
Split in half those intervals $[x_i,x_i-1]$ for which $i$ lies in $\widetilde{\mathcal{I}}$.
Update $n$, $\{x_i\}_{i=0}^n$, $\mathcal{I}$, and the $f[x_{i-1}, x_{i}, x_{i+1}]$ accordingly.  Return to Step~\ref{stage1}.
\end{enumerate}
\end{algoA}

we need a theorem that works.\\
\input{localcost}
Numerical examples.\\
We need computational cost\\
complexity bound \\

\section{The Minimization Algorithm, M}

we need a theorem that works.\\
Numerical examples.\\
We need computational cost\\
complexity bound \\

\section{Discussion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{References}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliography{FJH23,FJHOwn23}

\end{document}








